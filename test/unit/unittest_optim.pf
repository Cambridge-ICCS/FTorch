!| Unit tests for FTorch optimizer functionalities.
!
!  * License
!    FTorch is released under an MIT license.
!    See the [LICENSE](https://github.com/Cambridge-ICCS/FTorch/blob/main/LICENSE)
!    file for details.
module unittest_optim
  use funit
  use ftorch, only: assignment(=), operator(-), operator(*), operator(**), &
                    torch_kFloat32, torch_kCPU, torch_tensor, &
                    torch_delete, torch_tensor_from_array, torch_tensor_empty, &
                    torch_tensor_backward, torch_tensor_get_gradient
  use ftorch_optim, only: torch_optim, torch_optim_zero_grad, torch_optim_step, &
                          torch_optim_delete, torch_optim_SGD
  use ftorch_test_utils, only: assert_allclose
  use, intrinsic :: iso_c_binding, only: c_associated, c_int64_t
  use, intrinsic :: iso_fortran_env, only : sp => real32

  implicit none

  public

contains

  ! Unit test for deleting an optimizer by calling torch_optim_delete manually
  @test()
  subroutine test_torch_optim_delete()
    use ftorch, only: torch_tensor_ones

    implicit none

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor
    integer, parameter :: ndims = 2
    integer(c_int64_t), dimension(2), parameter :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32

    ! Check the optimizer pointer is not associated
    @assertFalse(c_associated(optimizer%p))
    ! Create a tensor to apply an optimizer to
    call torch_tensor_ones(tensor, ndims, tensor_shape, torch_kFloat32, torch_kCPU, &
                            requires_grad=.true.)
    ! Create an optimizer (we will use SGD in this example)
    call torch_optim_SGD(optimizer, [tensor])

    ! Check the optimizer pointer is now associated
    @assertTrue(c_associated(optimizer%p))

    ! Call torch_optim_delete manually
    call torch_optim_delete(optimizer)

    ! Check torch_optim_delete does indeed free the memory
    @assertFalse(c_associated(optimizer%p))

  end subroutine test_torch_optim_delete


  ! Unit test for zeroing gradients using torch_optim_zero_grad
  @test()
  subroutine test_torch_optim_zero_grad()
    use ftorch, only: torch_tensor_ones
    implicit none

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor, tensor_sq, grad
    integer, parameter :: ndims = 1
    integer, parameter :: tensor_layout(ndims) = [1]
    integer(c_int64_t), dimension(1), parameter :: tensor_shape = [4]
    real(sp), dimension(4) :: tensor_data = [1.0, 2.0, 3.0, 4.0]
    real(sp), dimension(4) :: grad_data
    real(sp), dimension(4) :: expected_zero = [0.0, 0.0, 0.0, 0.0]
    integer, parameter :: dtype = torch_kFloat32

    ! Create a tensor with requires_grad set to true
    call torch_tensor_from_array(tensor, tensor_data, tensor_layout, torch_kCPU, &
                                 requires_grad=.true.)
    call torch_tensor_from_array(grad, grad_data, tensor_layout, torch_kCPU)
    call torch_tensor_empty(tensor_sq, ndims, tensor_shape, torch_kFloat32, torch_kCPU)

    ! Create an optimizer (SGD with learning rate 1.0)
    call torch_optim_SGD(optimizer, [tensor])

    ! Perform a dummy backward pass to populate gradients
    tensor_sq = tensor ** 2
    call torch_tensor_backward(tensor_sq)

    ! Check if gradients are non-zero
    call torch_tensor_get_gradient(grad, tensor)
    @assertTrue(assert_allclose(grad_data, 2.0*tensor_data, test_name="test_torch_optim_zero_grad_1"))

    ! Zero the gradients
    call optimizer%zero_grad()

    ! Check if gradients are now zero
    call torch_tensor_get_gradient(grad, tensor)
    @assertTrue(assert_allclose(grad_data, expected_zero, test_name="test_torch_optim_zero_grad_2"))

    ! Clean up
    call torch_delete(tensor)
    call torch_delete(tensor_sq)
    call torch_delete(grad)
    call torch_optim_delete(optimizer)
  end subroutine test_torch_optim_zero_grad


  ! Unit test for stepping an optimizer
  ! Uses an optimizer with a learning rate of 1.0 such that a single step with gradients
  ! taken from a MSE loss should take us directly to the target
  @test()
  subroutine test_torch_optim_step()
    use ftorch, only: torch_tensor_ones
    implicit none

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor, target_tensor, loss, grad, torch_0p5
    integer, parameter :: ndims = 1
    integer, parameter :: tensor_layout(ndims) = [1]
    integer(c_int64_t), dimension(1), parameter :: tensor_shape = [4]
    real(sp), dimension(4) :: target_data = [1.0, 2.0, 3.0, 4.0]
    real(sp), dimension(4) :: tensor_data = [1.0, 1.0, 1.0, 1.0]
    integer, parameter :: dtype = torch_kFloat32

    ! Create a tensor with requires_grad set to true
    call torch_tensor_from_array(target_tensor, target_data, tensor_layout, torch_kCPU)
    call torch_tensor_from_array(tensor, tensor_data, tensor_layout, torch_kCPU, &
                            requires_grad=.true.)
    call torch_tensor_from_array(torch_0p5, [0.5_sp], tensor_layout, torch_kCPU)

    ! Create an optimizer (SGD with learning rate 1.0)
    call torch_optim_SGD(optimizer, [tensor], learning_rate=1D0)

    ! Calculate MSE loss between tensor and target and perform a backwards step
    call torch_tensor_empty(loss, ndims, tensor_shape, torch_kFloat32, torch_kCPU)
    loss = torch_0p5 * (tensor - target_tensor)**2
    call torch_tensor_backward(loss)

    ! Perform an optimizer step
    call optimizer%step()

    ! Check if the tensor values are updated correctly
    @assertEqual(tensor_data, target_data)

    ! Clean up
    call torch_delete(tensor)
    call torch_delete(target_tensor)
    call torch_delete(loss)
    call torch_optim_delete(optimizer)
  end subroutine test_torch_optim_step

end module unittest_optim
