!| Unit tests for FTorch optimizer functionalities.
!
!  * License
!    FTorch is released under an MIT license.
!    See the [LICENSE](https://github.com/Cambridge-ICCS/FTorch/blob/main/LICENSE)
!    file for details.
module unittest_optim
  use funit
  use ftorch, only: assignment(=), operator(-), operator(*), operator(**), &
                    torch_kFloat32, torch_kCPU, torch_tensor, &
                    torch_delete, torch_tensor_from_array, torch_tensor_empty, &
                    torch_tensor_backward, torch_tensor_get_gradient
  use ftorch_optim, only: torch_optim, torch_optim_zero_grad, torch_optim_step, &
                          torch_optim_delete, torch_optim_SGD, torch_optim_Adam, &
                          torch_optim_AdamW
  use ftorch_test_utils, only: assert_allclose
  use, intrinsic :: iso_c_binding, only: c_associated, c_int64_t
  use, intrinsic :: iso_fortran_env, only : sp => real32

  implicit none

  public

  @testParameter
  type, extends(AbstractTestParameter) :: OptimizerTestParameters
    character(len=:), allocatable :: optimizer_type
   contains
    procedure :: toString
  end type OptimizerTestParameters

  ! Typedef for a test case with a particular set of parameters
  @testCase(constructor=test_case_constructor)
  type, extends (ParameterizedTestCase) :: TestCaseType
    type(OptimizerTestParameters) :: param
  end type TestCaseType

contains

  ! Constructor for the test case type
  function test_case_constructor(param)
    type(TestCaseType) :: test_case_constructor
    type(OptimizerTestParameters), intent(in) :: param
    test_case_constructor%param = param
  end function test_case_constructor

  ! A fixture for varying the optimizer type
  function get_optimizer_parameters() result(params)
    type(OptimizerTestParameters), allocatable :: params(:)
    params = [ &
      OptimizerTestParameters("SGD"), &
      OptimizerTestParameters("Adam"), &
      OptimizerTestParameters("AdamW") &
    ]
  end function get_optimizer_parameters

  ! A fixture for writing out information about the specific parameters a test uses
function toString(this) result(string)
  class(OptimizerTestParameters), intent(in) :: this
  character(:), allocatable :: string

  if (allocated(this%optimizer_type)) then
    string = trim(this%optimizer_type)
  else
    string = "UNDEFINED"
  end if
end function toString

  ! Unit test for deleting an optimizer by calling torch_optim_delete manually
  @test(testParameters={get_optimizer_parameters()})
  subroutine test_torch_optim_delete(this)
    use ftorch, only: torch_tensor_ones

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor
    integer, parameter :: ndims = 2
    integer(c_int64_t), dimension(2), parameter :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32

    ! Check the optimizer pointer is not associated
    @assertFalse(c_associated(optimizer%p))

    ! Create a tensor to apply an optimizer to
    call torch_tensor_ones(tensor, ndims, tensor_shape, torch_kFloat32, torch_kCPU, &
                            requires_grad=.true.)

    ! Create an optimizer based on the parameter type
    if (this%param%optimizer_type == "SGD") then
      call torch_optim_SGD(optimizer, [tensor])
    else if (this%param%optimizer_type == "Adam") then
      call torch_optim_Adam(optimizer, [tensor])
    else if (this%param%optimizer_type == "AdamW") then
      call torch_optim_AdamW(optimizer, [tensor])
    else
      write(*,*) "Error :: unsupported optimizer type"
      stop 999
    end if

    ! Check the optimizer pointer is now associated
    @assertTrue(c_associated(optimizer%p))

    ! Call torch_optim_delete manually
    call torch_optim_delete(optimizer)

    ! Check torch_optim_delete does indeed free the memory
    @assertFalse(c_associated(optimizer%p))

  end subroutine test_torch_optim_delete


  ! Unit test for zeroing gradients using torch_optim_zero_grad
  @test(testParameters={get_optimizer_parameters()})
  subroutine test_torch_optim_zero_grad(this)
    use ftorch, only: torch_tensor_ones
    implicit none

    class(TestCaseType), intent(inout) :: this

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor, tensor_sq, grad
    integer, parameter :: ndims = 1
    integer, parameter :: tensor_layout(ndims) = [1]
    integer(c_int64_t), dimension(1), parameter :: tensor_shape = [4]
    real(sp), dimension(4) :: tensor_data, grad_data
    real(sp), dimension(4), parameter :: expected_zero = [0.0, 0.0, 0.0, 0.0]
    integer, parameter :: dtype = torch_kFloat32

    ! NOTE: When we call zero_grad libtorch does NOT set the gradients to 0.0, but
    ! rather makes them "undefined". Therefore we cannot test for gradient being 0.0,
    ! but rather check implicitly that gradients are 'refreshed' after calling a
    ! backward operation following zero_grad.
    ! To do this we make use of the retain_graph property of loss to check that this
    ! is enforced on a continuous graph, rather than re-creating the tensors each time.

    ! Initialise data for the test to avoid Fortran's implied save
    tensor_data = [1.0, 2.0, 3.0, 4.0]

    ! Create a tensor with requires_grad set to true
    call torch_tensor_from_array(tensor, tensor_data, tensor_layout, torch_kCPU, &
                                 requires_grad=.true.)
    call torch_tensor_from_array(grad, grad_data, tensor_layout, torch_kCPU)
    call torch_tensor_empty(tensor_sq, ndims, tensor_shape, torch_kFloat32, torch_kCPU)

    ! Create an optimizer based on the parameter type
    if (this%param%optimizer_type == "SGD") then
      call torch_optim_SGD(optimizer, [tensor])
    else if (this%param%optimizer_type == "Adam") then
      call torch_optim_Adam(optimizer, [tensor])
    else if (this%param%optimizer_type == "AdamW") then
      call torch_optim_AdamW(optimizer, [tensor])
    else
      write(*,*) "Error :: unsupported optimizer type"
      stop 999
    end if

    ! Perform a dummy backward pass to populate gradients
    tensor_sq = tensor ** 2
    call torch_tensor_backward(tensor_sq, retain_graph=.true.)

    ! Check gradients are non-zero (2*tensor)
    call torch_tensor_get_gradient(grad, tensor)
    @assertTrue(assert_allclose(grad_data, 2.0*tensor_data, test_name="test_torch_optim_zero_grad_1"))

    ! Repeat, and check that the gradients stack (2*2*tensor)
    tensor_sq = tensor ** 2
    call torch_tensor_backward(tensor_sq, retain_graph=.true.)
    call torch_tensor_get_gradient(grad, tensor)
    @assertTrue(assert_allclose(grad_data, 4.0*tensor_data, test_name="test_torch_optim_zero_grad_2"))

    ! Zero the gradients
    call optimizer%zero_grad()

    ! Repeat the operation but check gradients no longer stacked (back to 2*tensor)
    tensor_sq = tensor ** 2
    call torch_tensor_backward(tensor_sq, retain_graph=.true.)
    call torch_tensor_get_gradient(grad, tensor)
    @assertTrue(assert_allclose(grad_data, 2.0*tensor_data, test_name="test_torch_optim_zero_grad_3"))

    ! Clean up
    call torch_delete(tensor)
    call torch_delete(tensor_sq)
    call torch_delete(grad)
    call torch_optim_delete(optimizer)
  end subroutine test_torch_optim_zero_grad


  ! Unit test for stepping an optimizer
  ! Uses an optimizer with a learning rate of 1.0 such that a single step with gradients
  ! taken from a MSE loss should take us directly to the target
  ! See accompanying test_optim.py for PyTorch code.
  @test(testParameters={get_optimizer_parameters()})
  subroutine test_torch_optim_step(this)
    use ftorch, only: torch_tensor_ones, torch_tensor_mean
    implicit none

    class(TestCaseType), intent(inout) :: this

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor, target_tensor, loss, grad
    integer, parameter :: ndims = 1
    integer, parameter :: tensor_layout(ndims) = [1]
    integer(c_int64_t), dimension(1), parameter :: tensor_shape = [4]
    integer(c_int64_t), dimension(1), parameter :: scalar_shape = [1]
    real(sp), dimension(4) :: target_data, tensor_data, expected
    integer, parameter :: dtype = torch_kFloat32

    ! Initialise data for the test to avoid Fortran's implied save
    target_data = [1.0, 2.0, 3.0, 4.0]
    tensor_data = [1.0, 1.0, 1.0, 1.0]

    ! Create a tensor with requires_grad set to true
    call torch_tensor_from_array(target_tensor, target_data, tensor_layout, torch_kCPU)
    call torch_tensor_from_array(tensor, tensor_data, tensor_layout, torch_kCPU, &
                            requires_grad=.true.)

    ! Create an optimizer based on the parameter type, learning rate 1.0
    ! Also set the expected value of the optimizer step based on approximate value of
    ! calculation fron the appropriate formula.
    if (this%param%optimizer_type == "SGD") then
      call torch_optim_SGD(optimizer, [tensor], learning_rate=1.0D0)
      expected = [1.0D0, 1.5D0, 2.0D0, 2.5D0]
    else if (this%param%optimizer_type == "Adam") then
      call torch_optim_Adam(optimizer, [tensor], learning_rate=1.0D0)
      expected = [1.0D0, 2.0D0, 2.0D0, 2.0D0]
    else if (this%param%optimizer_type == "AdamW") then
      call torch_optim_AdamW(optimizer, [tensor], learning_rate=1.0D0)
      expected = [0.99D0, 1.99D0, 1.99D0, 1.99D0]
    else
      write(*,*) "Error :: unsupported optimizer type: ", this%param%optimizer_type
      stop 999
    end if

    ! Calculate MSE loss between tensor and target and perform a backwards step
    call torch_tensor_empty(loss, ndims, scalar_shape, torch_kFloat32, torch_kCPU)
    call torch_tensor_mean(loss, (tensor - target_tensor) ** 2)
    call torch_tensor_backward(loss)

    ! Perform an optimizer step
    call optimizer%step()

    ! Check if the tensor values are updated correctly
    if (.not. assert_allclose(tensor_data, expected, test_name="test_torch_optim_step ["//this%param%optimizer_type//"]")) then
      print *, "Error :: Incorrect value after applying optimizer step."
      print *, "Expected: ", expected
      print *, "     Got: ", tensor_data
      stop 999
    end if

    ! Clean up
    call torch_delete(tensor)
    call torch_delete(target_tensor)
    call torch_delete(loss)
    call torch_delete(grad)
    call torch_optim_delete(optimizer)
  end subroutine test_torch_optim_step

  ! Unit test for additional optimizer parameters
  ! Sets additional parameters and then compares the result of a single step to the
  ! values obtained for the same process through the PyTorch interface
  ! See accompanying test_optim.py for PyTorch code.
  @test(testParameters={get_optimizer_parameters()})
  subroutine test_torch_optim_params(this)
    use ftorch, only: torch_tensor_ones, torch_tensor_mean
    implicit none

    class(TestCaseType), intent(inout) :: this

    type(torch_optim) :: optimizer
    type(torch_tensor) :: tensor, target_tensor, loss, grad
    integer, parameter :: ndims = 1
    integer, parameter :: tensor_layout(ndims) = [1]
    integer(c_int64_t), dimension(1), parameter :: tensor_shape = [4]
    integer(c_int64_t), dimension(1), parameter :: scalar_shape = [1]
    real(sp), dimension(4) :: target_data, tensor_data, expected
    integer, parameter :: dtype = torch_kFloat32
    integer :: i

    ! Initialise data for the test to avoid Fortran's implied save
    target_data = [1.0, 2.0, 3.0, 4.0]
    tensor_data = [1.0, 1.0, 1.0, 1.0]

    ! Create a tensor with requires_grad set to true
    call torch_tensor_from_array(target_tensor, target_data, tensor_layout, torch_kCPU)
    call torch_tensor_from_array(tensor, tensor_data, tensor_layout, torch_kCPU, &
                            requires_grad=.true.)

    ! Create an optimizer based on the parameter type, learning rate 1.0
    ! Also set the expected value of the optimizer step based on approximate value of
    ! calculation fron the appropriate formula.
    if (this%param%optimizer_type == "SGD") then
      call torch_optim_SGD(optimizer, [tensor], learning_rate=0.1D0, momentum=0.9D0, &
                           weight_decay=0.025D0)
      expected = [0.979859D0, 1.840596D0, 2.701334D0, 3.562072D0]
    else if (this%param%optimizer_type == "Adam") then
      call torch_optim_Adam(optimizer, [tensor], learning_rate=0.1D0, beta_1=0.75D0, &
                            beta_2=0.8D0, weight_decay=0.025D0)
      expected = [1.067700D0, 1.389061D0, 1.388119D0, 1.387820D0]
    else if (this%param%optimizer_type == "AdamW") then
      call torch_optim_AdamW(optimizer, [tensor], learning_rate=0.1D0, beta_1=0.75D0, &
                            beta_2=0.8D0, weight_decay=0.025D0)
      expected = [0.919747D0, 1.378128D0, 1.376950D0, 1.376567D0]
    else
      write(*,*) "Error :: unsupported optimizer type: ", this%param%optimizer_type
      stop 999
    end if

    ! Run a loop of 4 iterations to capture aspects such as momentum
    do i = 1,4
      ! Calculate MSE loss between tensor and target and perform a backwards step
      call torch_tensor_empty(loss, ndims, scalar_shape, torch_kFloat32, torch_kCPU)
      call torch_tensor_mean(loss, (tensor - target_tensor) ** 2)
      call torch_tensor_backward(loss)

      ! Perform an optimizer step
      call optimizer%step()

      call torch_delete(loss)
    end do

    ! Check if the tensor values are updated correctly
    if (.not. assert_allclose(tensor_data, expected, test_name="test_torch_optim_params ["//this%param%optimizer_type//"]")) then
      print *, "Error :: Incorrect value after optimizer loop."
      print *, "Expected: ", expected
      print *, "     Got: ", tensor_data
      stop 999
    end if

    ! Clean up
    call torch_delete(tensor)
    call torch_delete(target_tensor)
    call torch_delete(grad)
    call torch_optim_delete(optimizer)
  end subroutine test_torch_optim_params

end module unittest_optim
