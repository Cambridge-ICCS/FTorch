!| Unit tests for FTorch's automatic differentiation of overloaded operators involving tensors.
!
!  * License
!    FTorch is released under an MIT license.
!    See the [LICENSE](https://github.com/Cambridge-ICCS/FTorch/blob/main/LICENSE)
!    file for details.
module test_tensor_operator_overloads_autograd
  use funit
  use ftorch, only: assignment(=), ftorch_int, torch_kCPU, torch_kFloat32, &
                    torch_tensor, torch_tensor_backward, torch_tensor_delete, torch_tensor_empty, &
                    torch_tensor_from_array, torch_tensor_ones, torch_tensor_get_gradient, &
                    torch_tensor_print
  use ftorch_test_utils, only: assert_allclose
  use, intrinsic :: iso_fortran_env, only: sp => real32
  use, intrinsic :: iso_c_binding, only : c_associated, c_int64_t

  implicit none

  public

  ! Set working precision for reals
  integer, parameter :: wp = sp

  ! All unit tests in this module run on CPU
  integer, parameter :: device_type = torch_kCPU

  ! Typedef holding a set of parameter values
  @testParameter
  type, extends(AbstractTestParameter) :: TestParametersType
    logical :: switch
  contains
    procedure :: toString
  end type TestParametersType

  ! Typedef for a test case with a particular set of parameters
  @testCase(constructor=test_case_ctor)
  type, extends (ParameterizedTestCase) :: TestCaseType
    type(TestParametersType) :: param
  end type TestCaseType

contains

  ! A fixture comprised of a full list of parameter sets
  function get_parameters_full() result(params)
    type(TestParametersType), allocatable :: params(:)
    params = [ &
      TestParametersType(.false.), &
      TestParametersType(.true.) &
    ]
  end function get_parameters_full

  ! A fixture comprised of a short list of parameter sets
  function get_parameters_short() result(params)
    type(TestParametersType), allocatable :: params(:)
    params = [TestParametersType(.false.)]
  end function get_parameters_short

  ! Constructor for the test case type
  function test_case_ctor(param)
    type(TestCaseType) :: test_case_ctor
    type(TestParametersType) :: param
    test_case_ctor%param = param
  end function test_case_ctor

  ! Function for representing a parameter set as a string
  function toString(this) result(string)
    class(TestParametersType), intent(in) :: this
    character(:), allocatable :: string
    character(len=1) :: str
    write(str,'(l1)') this%switch
    string = str
  end function toString

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_assign(this)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create a tensor based off an input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the first using the overloaded assignment
    ! operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Check the computed gradient takes the expected value:
    !   Q(a) = a => dQ/da = 1
    expected(:,:) = 1.0
    if (.not. assert_allclose(out_data, expected, test_name="test_torch_tensor_assign")) then
      print *, "Error :: incorrect gradient for assignment"
      stop 999
    end if

  end subroutine test_torch_tensor_assign

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_add(this)
    use ftorch, only: operator(+)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, b, dQda, dQdb
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data1, in_data2, out_data1, out_data2
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data1(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])
    in_data2(:,:) = reshape([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], [2, 3])

    ! Create tensor based off input arrays
    call torch_tensor_from_array(a, in_data1, tensor_layout, device_type, requires_grad=.true.)
    call torch_tensor_from_array(b, in_data2, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the sum of the first two using the overloaded
    ! addition operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a + b

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create tensors based off output arrays for the gradient and then retrieve them
    call torch_tensor_from_array(dQda, out_data1, tensor_layout, device_type)
    call torch_tensor_from_array(dQdb, out_data2, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)
    call torch_tensor_get_gradient(b, dQdb)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = a + b => dQ/da = 1
    expected(:,:) = 1.0
    if (.not. assert_allclose(out_data1, expected, test_name="test_torch_tensor_add1")) then
      print *, "Error :: incorrect gradient w.r.t. first input for addition"
      stop 999
    end if

    ! Extract Fortran array from the second computed gradient and its data with the expected value:
    !   Q(a,b) = a + b => dQ/db = 1
    expected(:,:) = 1.0
    if (.not. assert_allclose(out_data2, expected, test_name="test_torch_tensor_add2")) then
      print *, "Error :: incorrect gradient w.r.t. second input for addition"
      stop 999
    end if

  end subroutine test_torch_tensor_add

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_negative(this)
    use ftorch, only: operator(-)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create a tensor based off the input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the negative of the first one using the
    ! overloaded negation operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = -a

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Extract Fortran array from the computed gradient and its data with the expected value:
    !   Q(a) = a => dQ/da = -1
    expected(:,:) = -1.0
    if (.not. assert_allclose(out_data, expected, test_name="test_torch_tensor_negative")) then
      print *, "Error :: incorrect gradient for negation"
      stop 999
    end if

  end subroutine test_torch_tensor_negative

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_subtract(this)
    use ftorch, only: operator(-)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, b, dQda, dQdb
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data1, in_data2, out_data1, out_data2
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data1(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])
    in_data2(:,:) = reshape([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], [2, 3])

    ! Create tensor based off input arrays
    call torch_tensor_from_array(a, in_data1, tensor_layout, device_type, requires_grad=.true.)
    call torch_tensor_from_array(b, in_data2, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the difference of the first two using the
    ! overloaded subtraction operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a - b

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create tensors based off output arrays for the gradient and then retrieve them
    call torch_tensor_from_array(dQda, out_data1, tensor_layout, device_type)
    call torch_tensor_from_array(dQdb, out_data2, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)
    call torch_tensor_get_gradient(b, dQdb)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = a - b => dQ/da = 1
    expected(:,:) = 1.0
    if (.not. assert_allclose(out_data1, expected, test_name="test_torch_tensor_subtract1")) then
      print *, "Error :: incorrect gradient w.r.t. first input for subtraction"
      stop 999
    end if

    ! Extract Fortran array from the second computed gradient and its data with the expected value:
    !   Q(a,b) = a - b => dQ/db = -1
    expected(:,:) = -1.0
    if (.not. assert_allclose(out_data2, expected, test_name="test_torch_tensor_subtract2")) then
      print *, "Error :: incorrect gradient w.r.t. second input for subtraction"
      stop 999
    end if

  end subroutine test_torch_tensor_subtract

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_multiply(this)
    use ftorch, only: operator(*)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, b, dQda, dQdb
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data1, in_data2, out_data1, out_data2
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data1(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])
    in_data2(:,:) = reshape([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], [2, 3])

    ! Create tensor based off input arrays
    call torch_tensor_from_array(a, in_data1, tensor_layout, device_type, requires_grad=.true.)
    call torch_tensor_from_array(b, in_data2, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the product of the first two using the
    ! overloaded multiplication operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a * b

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create tensors based off output arrays for the gradient and then retrieve them
    call torch_tensor_from_array(dQda, out_data1, tensor_layout, device_type)
    call torch_tensor_from_array(dQdb, out_data2, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)
    call torch_tensor_get_gradient(b, dQdb)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = a * b => dQ/da = b
    expected(:,:) = in_data2
    if (.not. assert_allclose(out_data1, expected, test_name="test_torch_tensor_multiply1")) then
      print *, "Error :: incorrect gradient w.r.t. first input for multiplication"
      stop 999
    end if

    ! Extract Fortran array from the second computed gradient and its data with the expected value:
    !   Q(a,b) = a * b => dQ/db = a
    expected(:,:) = in_data1
    if (.not. assert_allclose(out_data2, expected, test_name="test_torch_tensor_multiply2")) then
      print *, "Error :: incorrect gradient w.r.t. second input for multiplication"
      stop 999
    end if

  end subroutine test_torch_tensor_multiply

  @test(testParameters={get_parameters_full()})
  subroutine test_torch_tensor_scalar_multiply(this)
    use ftorch, only: operator(*)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda, multiplier
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), parameter :: scalar = 3.14
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected
    logical :: test_pass

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create tensor based off input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create rank-1 tensor based off scalar
    call torch_tensor_from_array(multiplier, [scalar], tensor_layout, device_type)

    ! Create another empty tensors and assign it to the product of a scalar constant and the first
    ! tensor using the overloaded multiplication operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    if (this%param%switch) then
      Q = multiplier * a
    else
      Q = a * multiplier
    end if

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = scalar * a => dQ/da = scalar
    expected(:,:) = scalar
    test_pass = assert_allclose(out_data, expected, test_name="test_torch_tensor_scalar_multiply")
    if (.not. test_pass) then
      print *, "Error :: incorrect gradient for scalar multiplication"
      stop 999
    end if

  end subroutine test_torch_tensor_scalar_multiply

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_divide(this)
    use ftorch, only: operator(/)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, b, dQda, dQdb
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data1, in_data2, out_data1, out_data2
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data1(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])
    in_data2(:,:) = reshape([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], [2, 3])

    ! Create tensor based off input arrays
    call torch_tensor_from_array(a, in_data1, tensor_layout, device_type, requires_grad=.true.)
    call torch_tensor_from_array(b, in_data2, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the quotient of the first two using the
    ! overloaded division operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a / b

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create tensors based off output arrays for the gradient and then retrieve them
    call torch_tensor_from_array(dQda, out_data1, tensor_layout, device_type)
    call torch_tensor_from_array(dQdb, out_data2, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)
    call torch_tensor_get_gradient(b, dQdb)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = a / b => dQ/da = 1 / b
    expected(:,:) = 1.0 / in_data2
    if (.not. assert_allclose(out_data1, expected, test_name="test_torch_tensor_divide1")) then
      print *, "Error :: incorrect gradient w.r.t. numerator for division"
      stop 999
    end if

    ! Extract Fortran array from the second computed gradient and its data with the expected value:
    !   Q(a,b) = a / b => dQ/db = -a / b^2
    expected(:,:) = -in_data1 / in_data2 ** 2
    if (.not. assert_allclose(out_data2, expected, test_name="test_torch_tensor_divide2")) then
      print *, "Error :: incorrect gradient w.r.t. denominator for division"
      stop 999
    end if

  end subroutine test_torch_tensor_divide

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_scalar_divide(this)
    use ftorch, only: operator(/)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda, divisor
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), parameter :: scalar = 3.14
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected
    logical :: test_pass

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create tensor based off input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create rank-1 tensor based off scalar
    call torch_tensor_from_array(divisor, [scalar], tensor_layout, device_type)

    ! Create another empty tensors and assign it to the product of a scalar constant and the first
    ! tensor using the overloaded multiplication operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a / divisor

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Extract Fortran array from the first computed gradient and its data with the expected value:
    !   Q(a,b) = a / scalar => dQ/da = 1 / scalar
    expected(:,:) = 1.0 / scalar
    test_pass = assert_allclose(out_data, expected, test_name="test_torch_tensor_scalar_divide")
    if (.not. test_pass) then
      print *, "Error :: incorrect gradient for scalar division"
      stop 999
    end if

  end subroutine test_torch_tensor_scalar_divide

  @test(testParameters={get_parameters_full()})
  subroutine test_torch_tensor_square(this)
    use ftorch, only: operator(**)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create a tensor based off the input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the square of the first one using the
    ! overloaded power operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    if (this%param%switch) then
      Q = a ** 2.0
    else
      Q = a ** 2
    end if

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Extract Fortran array from the computed gradient and its data with the expected value:
    !   Q(a) = a^2 => dQ/da = 2 * a
    expected(:,:) = 2.0 * in_data
    if (.not. assert_allclose(out_data, expected, test_name="test_torch_tensor_square")) then
      print *, "Error :: incorrect gradient for square"
      stop 999
    end if

  end subroutine test_torch_tensor_square

  @test(testParameters={get_parameters_short()})
  subroutine test_torch_tensor_sqrt(this)
    use ftorch, only: operator(**)

    implicit none

    class(TestCaseType), intent(inout) :: this
    type(torch_tensor) :: Q, a, dQda
    integer, parameter :: ndims = 2
    integer(ftorch_int), parameter :: tensor_layout(ndims) = [1, 2]
    integer(c_int64_t), parameter, dimension(ndims) :: tensor_shape = [2, 3]
    integer, parameter :: dtype = torch_kFloat32
    real(wp), dimension(2,3), target :: in_data, out_data
    real(wp), dimension(2,3) :: expected

    ! Create an arbitrary input array
    in_data(:,:) = reshape([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], [2, 3])

    ! Create a tensor based off the input array
    call torch_tensor_from_array(a, in_data, tensor_layout, device_type, requires_grad=.true.)

    ! Create another empty tensor and assign it to the square root of the first one using the
    ! overloaded power operator
    call torch_tensor_empty(Q, ndims, tensor_shape, dtype, device_type)
    Q = a ** 0.5

    ! Apply back-propagation
    call torch_tensor_backward(Q)

    ! Create another tensor based off an output array for the gradient and then retrieve it
    call torch_tensor_from_array(dQda, out_data, tensor_layout, device_type)
    call torch_tensor_get_gradient(a, dQda)

    ! Extract Fortran array from the computed gradient and its data with the expected value:
    !   Q(a) = a^{1/2} => dQ/da = 0.5 * a^{-1/2})
    expected(:,:) = 0.5 / in_data ** 0.5
    if (.not. assert_allclose(out_data, expected, test_name="test_torch_tensor_sqrt")) then
      print *, "Error :: incorrect gradient for square root"
      stop 999
    end if

  end subroutine test_torch_tensor_sqrt

end module test_tensor_operator_overloads_autograd
