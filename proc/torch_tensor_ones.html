<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="A library for coupling (Py)Torch machine learning models to Fortran">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>torch_tensor_ones &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="../page/index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>torch_tensor_ones
      <small>Subroutine</small>
      
    </h1>
      <div class="container p-2 mb-4 bg-light border rounded-3">
    <div class="row align-items-center justify-content-between" id="info-bar">
      <div class="col">
        <ul class="list-inline" style="margin-bottom:0px;display:inline">
            <li class="list-inline-item" id="meta-license"><i class="fa fa-legal"></i> <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a></li>

            <li class="list-inline-item" id="statements"><i class="fa fa-list-ol"></i>
              <a data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-html="true"
                 title=" 1.8% of total for procedures.">38 statements</a>
            </li>

            <li class="list-inline-item" id="source-file">
              <i class="fa fa-code"></i>
              <a href="../src/ftorch.F90"> Source File</a>
            </li>
        </ul>
      </div>
      <div class="col">
        <nav aria-label="breadcrumb">
          <ol class="breadcrumb justify-content-end mb-0">
                <li class="breadcrumb-item"><a href='../sourcefile/ftorch.f90.html'>ftorch.F90</a></li>
                <li class="breadcrumb-item"><a href='../module/ftorch.html'>ftorch</a></li>
            <li class="breadcrumb-item active" aria-current="page">torch_tensor_ones</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
  <script>
    // Enable Bootstrap tooltips
    (function () {
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
    })();
  </script>

  </div>
  
  <div class="row">
    <div class="col-md-3 hidden-xs hidden-sm visible-md visible-lg">
      <div id="sidebar">
      <h3>Contents</h3>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    <div class="card card-primary">
      <div class="card-header text-left"><h3 class="card-title">Source Code</h3></div>
      <div class="list-group">
        <a class="list-group-item" href="../proc/torch_tensor_ones.html#src">torch_tensor_ones</a>
      </div>
    </div>


  </div>

    </div>
    
    <div class="col-md-9" id='text'>
    <h2>public  subroutine torch_tensor_ones(tensor, ndims, tensor_shape, dtype, device_type, device_index, requires_grad)  
</h2>
        <div class="card mb-4">
      <h3 class="card-header card-title bg-light">Uses</h3>
      <div class="card-body">
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
              <ul class="list-inline">
                  <li class="list-inline-item"><a href='http://fortranwiki.org/fortran/show/iso_c_binding'>iso_c_binding</a></li>
              </ul>
            </li>
            <li class="list-group-item">
              <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: proc~~torch_tensor_ones~~UsesGraph Pages: 1 -->
<svg id="proctorch_tensor_onesUsesGraph" width="237pt" height="32pt"
 viewBox="0.00 0.00 237.00 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="proc~~torch_tensor_ones~~UsesGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>proc~~torch_tensor_ones~~UsesGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 233,-28 233,4 -4,4"/>
<!-- proc~torch_tensor_ones -->
<g id="proc~~torch_tensor_ones~~UsesGraph_node1" class="node">
<title>proc~torch_tensor_ones</title>
<polygon fill="none" stroke="black" points="229,-24 120,-24 120,0 229,0 229,-24"/>
<text text-anchor="middle" x="174.5" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">torch_tensor_ones</text>
</g>
<!-- iso_c_binding -->
<g id="proc~~torch_tensor_ones~~UsesGraph_node2" class="node">
<title>iso_c_binding</title>
<g id="a_proc~~torch_tensor_ones~~UsesGraph_node2"><a xlink:href="http://fortranwiki.org/fortran/show/iso_c_binding" xlink:title="iso_c_binding">
<polygon fill="#337ab7" stroke="#337ab7" points="84,-24 0,-24 0,0 84,0 84,-24"/>
<text text-anchor="middle" x="42" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">iso_c_binding</text>
</a>
</g>
</g>
<!-- proc~torch_tensor_ones&#45;&gt;iso_c_binding -->
<g id="proc~~torch_tensor_ones~~UsesGraph_edge1" class="edge">
<title>proc~torch_tensor_ones&#45;&gt;iso_c_binding</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M119.67,-12C111.37,-12 102.83,-12 94.58,-12"/>
<polygon fill="#000000" stroke="#000000" points="94.35,-8.5 84.35,-12 94.35,-15.5 94.35,-8.5"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#UsesGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="UsesGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="518pt" height="32pt"
 viewBox="0.00 0.00 517.50 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 513.5,-28 513.5,4 -4,4"/>
<!-- Module -->
<g id="node1" class="node">
<title>Module</title>
<polygon fill="#337ab7" stroke="#337ab7" points="54,-24 0,-24 0,0 54,0 54,-24"/>
<text text-anchor="middle" x="27" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Module</text>
</g>
<!-- Submodule -->
<g id="node2" class="node">
<title>Submodule</title>
<polygon fill="#5bc0de" stroke="#5bc0de" points="145.5,-24 72.5,-24 72.5,0 145.5,0 145.5,-24"/>
<text text-anchor="middle" x="109" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Submodule</text>
</g>
<!-- Subroutine -->
<g id="node3" class="node">
<title>Subroutine</title>
<polygon fill="#d9534f" stroke="#d9534f" points="234,-24 164,-24 164,0 234,0 234,-24"/>
<text text-anchor="middle" x="199" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Subroutine</text>
</g>
<!-- Function -->
<g id="node4" class="node">
<title>Function</title>
<polygon fill="#d94e8f" stroke="#d94e8f" points="310,-24 252,-24 252,0 310,0 310,-24"/>
<text text-anchor="middle" x="281" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Function</text>
</g>
<!-- Program -->
<g id="node5" class="node">
<title>Program</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="386,-24 328,-24 328,0 386,0 386,-24"/>
<text text-anchor="middle" x="357" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Program</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node6" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="509.5,-24 404.5,-24 404.5,0 509.5,0 509.5,-24"/>
<text text-anchor="middle" x="457" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a submodule to the (sub)module which it is
descended from. Dashed arrows point from a module or program unit to 
modules which it uses.
</p>
 </div>
            </div>
          </div>
        </div>
            </li>
        </ul>
      </div>
    </div>


    <p>Returns a tensor filled with the scalar value 1.</p>


    <h3>Arguments</h3>
        <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
                <p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
                <p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~4"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
                <p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
                <p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
                <p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~4"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
                <p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~5"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
                <p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>

    <br>
    <div class="card">
      <div class="card-header">
  <h3 class="card-title">Called by</h3>
      </div>
      <div class="card-body">
  <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: proc~~torch_tensor_ones~~CalledByGraph Pages: 1 -->
<svg id="proctorch_tensor_onesCalledByGraph" width="289pt" height="32pt"
 viewBox="0.00 0.00 289.00 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="proc~~torch_tensor_ones~~CalledByGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>proc~~torch_tensor_ones~~CalledByGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 285,-28 285,4 -4,4"/>
<!-- proc~torch_tensor_ones -->
<g id="proc~~torch_tensor_ones~~CalledByGraph_node1" class="node">
<title>proc~torch_tensor_ones</title>
<polygon fill="none" stroke="black" points="281,-24 172,-24 172,0 281,0 281,-24"/>
<text text-anchor="middle" x="226.5" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">torch_tensor_ones</text>
</g>
<!-- proc~torch_tensor_backward -->
<g id="proc~~torch_tensor_ones~~CalledByGraph_node2" class="node">
<title>proc~torch_tensor_backward</title>
<g id="a_proc~~torch_tensor_ones~~CalledByGraph_node2"><a xlink:href="../proc/torch_tensor_backward.html" xlink:title="torch_tensor_backward">
<polygon fill="#d9534f" stroke="#d9534f" points="136,-24 0,-24 0,0 136,0 136,-24"/>
<text text-anchor="middle" x="68" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">torch_tensor_backward</text>
</a>
</g>
</g>
<!-- proc~torch_tensor_backward&#45;&gt;proc~torch_tensor_ones -->
<g id="proc~~torch_tensor_ones~~CalledByGraph_edge1" class="edge">
<title>proc~torch_tensor_backward&#45;&gt;proc~torch_tensor_ones</title>
<path fill="none" stroke="#000000" d="M136.28,-12C144.76,-12 153.42,-12 161.84,-12"/>
<polygon fill="#000000" stroke="#000000" points="161.97,-15.5 171.97,-12 161.97,-8.5 161.97,-15.5"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#CalledByGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="CalledByGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="641pt" height="28pt"
 viewBox="0.00 0.00 641.00 27.51" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(0.86 0.86) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 741.5,-28 741.5,4 -4,4"/>
<!-- Subroutine -->
<g id="node1" class="node">
<title>Subroutine</title>
<polygon fill="#d9534f" stroke="#d9534f" points="70,-24 0,-24 0,0 70,0 70,-24"/>
<text text-anchor="middle" x="35" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Subroutine</text>
</g>
<!-- Function -->
<g id="node2" class="node">
<title>Function</title>
<polygon fill="#d94e8f" stroke="#d94e8f" points="146,-24 88,-24 88,0 146,0 146,-24"/>
<text text-anchor="middle" x="117" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Function</text>
</g>
<!-- Interface -->
<g id="node3" class="node">
<title>Interface</title>
<polygon fill="#a7506f" stroke="#a7506f" points="225.5,-24 164.5,-24 164.5,0 225.5,0 225.5,-24"/>
<text text-anchor="middle" x="195" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Interface</text>
</g>
<!-- Type Bound Procedure -->
<g id="node4" class="node">
<title>Type Bound Procedure</title>
<polygon fill="#a7506f" stroke="#a7506f" points="374,-24 244,-24 244,0 374,0 374,-24"/>
<text text-anchor="middle" x="309" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Type Bound Procedure</text>
</g>
<!-- Unknown Procedure Type -->
<g id="node5" class="node">
<title>Unknown Procedure Type</title>
<polygon fill="#777777" stroke="#777777" points="537.5,-24 392.5,-24 392.5,0 537.5,0 537.5,-24"/>
<text text-anchor="middle" x="465" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Unknown Procedure Type</text>
</g>
<!-- Program -->
<g id="node6" class="node">
<title>Program</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="614,-24 556,-24 556,0 614,0 614,-24"/>
<text text-anchor="middle" x="585" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Program</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node7" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="737.5,-24 632.5,-24 632.5,0 737.5,0 737.5,-24"/>
<text text-anchor="middle" x="685" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a procedure to one which it calls. Dashed 
arrows point from an interface to procedures which implement that interface.
This could include the module procedures in a generic interface or the
implementation in a submodule of an interface in a parent module.
</p>
 </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br>


    
    

    
    



    
    <section>
    <h2><span class="anchor" id="src"></span>Source Code</h2>
    <div class="hl codehilite"><pre><span></span><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_tensor_ones</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                               </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device_index</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">)</span>
<span class="w">    </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int64_t</span>
<span class="kt">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor</span><span class="w">     </span><span class="c">!! Returned tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">ndims</span><span class="w">      </span><span class="c">!! Number of dimensions of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int64_t</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">  </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">(:)</span><span class="w">   </span><span class="c">!! Shape of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">dtype</span><span class="w">        </span><span class="c">!! Data type of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type</span><span class="w">  </span><span class="c">!! Device type the tensor will live on (`torch_kCPU` or `torch_kCUDA`)</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index</span><span class="w">   </span><span class="c">!! Device index to use for `torch_kCUDA` case</span>
<span class="w">    </span><span class="kt">logical</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">)</span><span class="w">                  </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_value</span><span class="w">    </span><span class="c">!! device index used</span>
<span class="w">    </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">)</span><span class="w">                 </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="w">   </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>

<span class="w">    </span><span class="k">interface</span>
<span class="k">      function </span><span class="n">torch_ones_c</span><span class="p">(</span><span class="n">ndims_c</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape_c</span><span class="p">,</span><span class="w"> </span><span class="n">dtype_c</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                            </span><span class="n">device_type_c</span><span class="p">,</span><span class="w"> </span><span class="n">device_index_c</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad_c</span><span class="p">)</span><span class="w"> </span><span class="k">result</span><span class="p">(</span><span class="n">tensor_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_ones&#39;</span><span class="p">)</span>
<span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int64_t</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span>

<span class="kt">        </span><span class="k">implicit none</span>

<span class="k">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">ndims_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int64_t</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">    </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_shape_c</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">dtype_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_c</span>
<span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_c</span>
<span class="w">        </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">)</span><span class="w">                       </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_c</span>
<span class="w">      </span><span class="k">end function </span><span class="n">torch_ones_c</span>
<span class="w">    </span><span class="k">end interface</span>

<span class="w">    </span><span class="c">! Process optional arguments</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">present</span><span class="p">(</span><span class="n">device_index</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_index</span>
<span class="w">    </span><span class="k">else if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span>
<span class="w">    </span><span class="k">else</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">endif</span>

<span class="k">    if</span><span class="w"> </span><span class="p">(.</span><span class="nb">not</span><span class="p">.</span><span class="w"> </span><span class="nb">present</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">logical</span><span class="p">(.</span><span class="n">false</span><span class="p">.,</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span>
<span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span>
<span class="w">    </span><span class="k">end if</span>

<span class="k">    </span><span class="n">tensor</span><span class="p">%</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch_ones_c</span><span class="p">(</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w">           </span><span class="p">&amp;</span>
<span class="w">                            </span><span class="n">device_index_value</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="p">)</span>
<span class="w">  </span><span class="k">end subroutine </span><span class="n">torch_tensor_ones</span>
</pre></div>

    </section>
    <br>
    
    </div>
  </div>

      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2025 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>