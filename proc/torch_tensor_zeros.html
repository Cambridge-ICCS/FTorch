<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="FTorch - A library for coupling (Py)Torch machine learning models to Fortran codes.Written in modern Fortran (2008) with source code available on GitHub it has been used in multiple scientific projects.The associated JOSS paper can read here.">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>torch_tensor_zeros &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
      <link href="../css/user.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="../page/index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>torch_tensor_zeros
      <small>Subroutine</small>
      
    </h1>
      <div class="container p-2 mb-4 bg-light border rounded-3">
    <div class="row align-items-center justify-content-between" id="info-bar">
      <div class="col">
        <ul class="list-inline" style="margin-bottom:0px;display:inline">
            <li class="list-inline-item" id="meta-license"><i class="fa fa-legal"></i> <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a></li>

            <li class="list-inline-item" id="statements"><i class="fa fa-list-ol"></i>
              <a data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-html="true"
                 title=" 1.8% of total for procedures.">38 statements</a>
            </li>

            <li class="list-inline-item" id="source-file">
              <i class="fa fa-code"></i>
              <a href="../src/ftorch.F90"> Source File</a>
            </li>
        </ul>
      </div>
      <div class="col">
        <nav aria-label="breadcrumb">
          <ol class="breadcrumb justify-content-end mb-0">
                <li class="breadcrumb-item"><a href='../sourcefile/ftorch.f90.html'>ftorch.F90</a></li>
                <li class="breadcrumb-item"><a href='../module/ftorch.html'>ftorch</a></li>
            <li class="breadcrumb-item active" aria-current="page">torch_tensor_zeros</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
  <script>
    // Enable Bootstrap tooltips
    (function () {
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
    })();
  </script>

  </div>
  
  <div class="row">
    <div class="col-md-3 hidden-xs hidden-sm visible-md visible-lg">
      <div id="sidebar">
      <h3>Contents</h3>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    <div class="card card-primary">
      <div class="card-header text-left"><h3 class="card-title">Source Code</h3></div>
      <div class="list-group">
        <a class="list-group-item" href="../proc/torch_tensor_zeros.html#src">torch_tensor_zeros</a>
      </div>
    </div>


  </div>

    </div>
    
    <div class="col-md-9" id='text'>
    <h2>public  subroutine torch_tensor_zeros(tensor, ndims, tensor_shape, dtype, device_type, device_index, requires_grad)  
</h2>
        <div class="card mb-4">
      <h3 class="card-header card-title bg-light">Uses</h3>
      <div class="card-body">
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
              <ul class="list-inline">
                  <li class="list-inline-item"><a href='http://fortranwiki.org/fortran/show/iso_c_binding'>iso_c_binding</a></li>
              </ul>
            </li>
            <li class="list-group-item">
              <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: proc~~torch_tensor_zeros~~UsesGraph Pages: 1 -->
<svg id="proctorch_tensor_zerosUsesGraph" width="240pt" height="32pt"
 viewBox="0.00 0.00 240.00 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="proc~~torch_tensor_zeros~~UsesGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>proc~~torch_tensor_zeros~~UsesGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 236,-28 236,4 -4,4"/>
<!-- proc~torch_tensor_zeros -->
<g id="proc~~torch_tensor_zeros~~UsesGraph_node1" class="node">
<title>proc~torch_tensor_zeros</title>
<polygon fill="none" stroke="black" points="232,-24 120,-24 120,0 232,0 232,-24"/>
<text text-anchor="middle" x="176" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">torch_tensor_zeros</text>
</g>
<!-- iso_c_binding -->
<g id="proc~~torch_tensor_zeros~~UsesGraph_node2" class="node">
<title>iso_c_binding</title>
<g id="a_proc~~torch_tensor_zeros~~UsesGraph_node2"><a xlink:href="http://fortranwiki.org/fortran/show/iso_c_binding" xlink:title="iso_c_binding">
<polygon fill="#337ab7" stroke="#337ab7" points="84,-24 0,-24 0,0 84,0 84,-24"/>
<text text-anchor="middle" x="42" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">iso_c_binding</text>
</a>
</g>
</g>
<!-- proc~torch_tensor_zeros&#45;&gt;iso_c_binding -->
<g id="proc~~torch_tensor_zeros~~UsesGraph_edge1" class="edge">
<title>proc~torch_tensor_zeros&#45;&gt;iso_c_binding</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M119.78,-12C111.44,-12 102.86,-12 94.59,-12"/>
<polygon fill="#000000" stroke="#000000" points="94.34,-8.5 84.34,-12 94.34,-15.5 94.34,-8.5"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#UsesGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="UsesGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="518pt" height="32pt"
 viewBox="0.00 0.00 517.50 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 513.5,-28 513.5,4 -4,4"/>
<!-- Module -->
<g id="node1" class="node">
<title>Module</title>
<polygon fill="#337ab7" stroke="#337ab7" points="54,-24 0,-24 0,0 54,0 54,-24"/>
<text text-anchor="middle" x="27" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Module</text>
</g>
<!-- Submodule -->
<g id="node2" class="node">
<title>Submodule</title>
<polygon fill="#5bc0de" stroke="#5bc0de" points="145.5,-24 72.5,-24 72.5,0 145.5,0 145.5,-24"/>
<text text-anchor="middle" x="109" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Submodule</text>
</g>
<!-- Subroutine -->
<g id="node3" class="node">
<title>Subroutine</title>
<polygon fill="#d9534f" stroke="#d9534f" points="234,-24 164,-24 164,0 234,0 234,-24"/>
<text text-anchor="middle" x="199" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Subroutine</text>
</g>
<!-- Function -->
<g id="node4" class="node">
<title>Function</title>
<polygon fill="#d94e8f" stroke="#d94e8f" points="310,-24 252,-24 252,0 310,0 310,-24"/>
<text text-anchor="middle" x="281" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Function</text>
</g>
<!-- Program -->
<g id="node5" class="node">
<title>Program</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="386,-24 328,-24 328,0 386,0 386,-24"/>
<text text-anchor="middle" x="357" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Program</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node6" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="509.5,-24 404.5,-24 404.5,0 509.5,0 509.5,-24"/>
<text text-anchor="middle" x="457" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a submodule to the (sub)module which it is
descended from. Dashed arrows point from a module or program unit to 
modules which it uses.
</p>
 </div>
            </div>
          </div>
        </div>
            </li>
        </ul>
      </div>
    </div>


    <p>Returns a tensor filled with the scalar value 0.</p>


    <h3>Arguments</h3>
        <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
                <p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
                <p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~3"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
                <p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
                <p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
                <p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~3"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
                <p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~4"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
                <p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>

    <br>


    
    

    
    



    
    <section>
    <h2><span class="anchor" id="src"></span>Source Code</h2>
    <div class="hl codehilite"><pre><span></span><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_tensor_zeros</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="w"> </span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                                </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device_index</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">)</span>
<span class="w">    </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int64_t</span>
<span class="kt">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor</span><span class="w">     </span><span class="c">!! Returned tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">ndims</span><span class="w">      </span><span class="c">!! Number of dimensions of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int64_t</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">  </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">(:)</span><span class="w">   </span><span class="c">!! Shape of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">dtype</span><span class="w">      </span><span class="c">!! Data type of the tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type</span><span class="w">  </span><span class="c">!! Device type the tensor will live on (`torch_kCPU` or `torch_kCUDA`)</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index</span><span class="w">   </span><span class="c">!! Device index to use for `torch_kCUDA` case</span>
<span class="w">    </span><span class="kt">logical</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">)</span><span class="w">                  </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_value</span><span class="w">   </span><span class="c">!! device index used</span>
<span class="w">    </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">)</span><span class="w">                 </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>

<span class="w">    </span><span class="k">interface</span>
<span class="k">      function </span><span class="n">torch_zeros_c</span><span class="p">(</span><span class="n">ndims_c</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape_c</span><span class="p">,</span><span class="w"> </span><span class="n">dtype_c</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                             </span><span class="n">device_type_c</span><span class="p">,</span><span class="w"> </span><span class="n">device_index_c</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad_c</span><span class="p">)</span><span class="w"> </span><span class="k">result</span><span class="p">(</span><span class="n">tensor_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_zeros&#39;</span><span class="p">)</span>
<span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int64_t</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span>

<span class="kt">        </span><span class="k">implicit none</span>

<span class="k">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">ndims_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int64_t</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">    </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_shape_c</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">dtype_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type_c</span>
<span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_c</span>
<span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_c</span>
<span class="w">        </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">)</span><span class="w">                       </span><span class="kd">::</span><span class="w"> </span><span class="n">tensor_c</span>
<span class="w">      </span><span class="k">end function </span><span class="n">torch_zeros_c</span>
<span class="w">    </span><span class="k">end interface</span>

<span class="w">    </span><span class="c">! Process optional arguments</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">present</span><span class="p">(</span><span class="n">device_index</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_index</span>
<span class="w">    </span><span class="k">else if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span>
<span class="w">    </span><span class="k">else</span>
<span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">endif</span>

<span class="k">    if</span><span class="w"> </span><span class="p">(.</span><span class="nb">not</span><span class="p">.</span><span class="w"> </span><span class="nb">present</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">logical</span><span class="p">(.</span><span class="n">false</span><span class="p">.,</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">)</span>
<span class="w">    </span><span class="k">else</span>
<span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span>
<span class="w">    </span><span class="k">end if</span>

<span class="k">    </span><span class="n">tensor</span><span class="p">%</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch_zeros_c</span><span class="p">(</span><span class="n">ndims</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_shape</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w">          </span><span class="p">&amp;</span>
<span class="w">                             </span><span class="n">device_index_value</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="p">)</span>
<span class="w">  </span><span class="k">end subroutine </span><span class="n">torch_tensor_zeros</span>
</pre></div>

    </section>
    <br>
    
    </div>
  </div>

      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2026 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>