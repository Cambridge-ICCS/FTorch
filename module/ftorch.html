<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="A library for coupling (Py)Torch machine learning models to Fortran">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>ftorch &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="../page/index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>ftorch
      <small>Module</small>
      
    </h1>
      <div class="container p-2 mb-4 bg-light border rounded-3">
    <div class="row align-items-center justify-content-between" id="info-bar">
      <div class="col">
        <ul class="list-inline" style="margin-bottom:0px;display:inline">
            <li class="list-inline-item" id="meta-license"><i class="fa fa-legal"></i> <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a></li>

            <li class="list-inline-item" id="statements"><i class="fa fa-list-ol"></i>
              <a data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-html="true"
                 title="86.9% of total for modules and submodules.">1853 statements</a>
            </li>

            <li class="list-inline-item" id="source-file">
              <i class="fa fa-code"></i>
              <a href="../src/ftorch.F90"> Source File</a>
            </li>
        </ul>
      </div>
      <div class="col">
        <nav aria-label="breadcrumb">
          <ol class="breadcrumb justify-content-end mb-0">
                <li class="breadcrumb-item"><a href='../sourcefile/ftorch.f90.html'>ftorch.F90</a></li>
            <li class="breadcrumb-item active" aria-current="page">ftorch</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
  <script>
    // Enable Bootstrap tooltips
    (function () {
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
    })();
  </script>

  </div>

  <div class="row">
    <div class="col-md-3">
        <div id="sidebar">
      <h3>Contents</h3>
  
      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#vars-0"
         aria-expanded="false" aria-controls="vars-0">
         <h4 class="card-header bg-primary text-white">Variables</h4>
      </a>
      <div id="vars-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#variable-ftorch_int">ftorch_int</a>
        </div>
      </div>
    </div>

      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#enums-0"
         aria-expanded="false" aria-controls="enums-0">
         <h4 class="card-header bg-primary text-white">Enumerations</h4>
      </a>
      <div id="enums-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#enum-__unnamed__"><em>unnamed</em></a>
            <a class="list-group-item" href="../module/ftorch.html#enum-__unnamed__~2"><em>unnamed</em></a>
        </div>
      </div>
    </div>

  
  
  
  
  
      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#inters-0"
         aria-expanded="false" aria-controls="inters-0">
         <h4 class="card-header bg-primary text-white">Interfaces</h4>
      </a>
      <div id="inters-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#interface-assignment%20%28%3D%29">assignment (=)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-operator%20%28ASTERISK%29">operator (*)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-operator%20%28ASTERISKASTERISK%29">operator (**)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-operator%20%28%2B%29">operator (+)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-operator%20%28-%29">operator (-)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-operator%20%28SLASH%29">operator (/)</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-torch_delete">torch_delete</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-torch_from_blob_c">torch_from_blob_c</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-torch_tensor_divide_c">torch_tensor_divide_c</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-torch_tensor_from_array">torch_tensor_from_array</a>
            <a class="list-group-item" href="../module/ftorch.html#interface-torch_tensor_multiply_c">torch_tensor_multiply_c</a>
        </div>
      </div>
    </div>

  
      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#types-0"
         aria-expanded="false" aria-controls="types-0">
         <h4 class="card-header bg-primary text-white">Derived Types</h4>
      </a>
      <div id="types-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#type-torch_model">torch_model</a>
            <a class="list-group-item" href="../module/ftorch.html#type-torch_tensor">torch_tensor</a>
        </div>
      </div>
    </div>

      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#funcs-0"
         aria-expanded="false" aria-controls="funcs-0">
         <h4 class="card-header bg-primary text-white">Functions</h4>
      </a>
      <div id="funcs-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_add">torch_tensor_add</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_divide">torch_tensor_divide</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_device_index">torch_tensor_get_device_index</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_device_type">torch_tensor_get_device_type</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_dtype">torch_tensor_get_dtype</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_rank">torch_tensor_get_rank</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_shape">torch_tensor_get_shape</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_multiply">torch_tensor_multiply</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_negative">torch_tensor_negative</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_int16">torch_tensor_power_int16</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_int32">torch_tensor_power_int32</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_int64">torch_tensor_power_int64</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_int8">torch_tensor_power_int8</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_real32">torch_tensor_power_real32</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_power_real64">torch_tensor_power_real64</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_requires_grad">torch_tensor_requires_grad</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_subtract">torch_tensor_subtract</a>
        </div>
      </div>
    </div>

      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#subs-0"
         aria-expanded="false" aria-controls="subs-0">
         <h4 class="card-header bg-primary text-white">Subroutines</h4>
      </a>
      <div id="subs-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_model_delete">torch_model_delete</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_model_forward">torch_model_forward</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_model_load">torch_model_load</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_array_delete">torch_tensor_array_delete</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_assign">torch_tensor_assign</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_backward">torch_tensor_backward</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_delete">torch_tensor_delete</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_empty">torch_tensor_empty</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_1d">torch_tensor_from_array_int16_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_1d_default_layout">torch_tensor_from_array_int16_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_2d">torch_tensor_from_array_int16_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_2d_default_layout">torch_tensor_from_array_int16_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_3d">torch_tensor_from_array_int16_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_3d_default_layout">torch_tensor_from_array_int16_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_4d">torch_tensor_from_array_int16_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_4d_default_layout">torch_tensor_from_array_int16_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_5d">torch_tensor_from_array_int16_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int16_5d_default_layout">torch_tensor_from_array_int16_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_1d">torch_tensor_from_array_int32_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_1d_default_layout">torch_tensor_from_array_int32_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_2d">torch_tensor_from_array_int32_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_2d_default_layout">torch_tensor_from_array_int32_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_3d">torch_tensor_from_array_int32_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_3d_default_layout">torch_tensor_from_array_int32_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_4d">torch_tensor_from_array_int32_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_4d_default_layout">torch_tensor_from_array_int32_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_5d">torch_tensor_from_array_int32_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int32_5d_default_layout">torch_tensor_from_array_int32_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_1d">torch_tensor_from_array_int64_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_1d_default_layout">torch_tensor_from_array_int64_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_2d">torch_tensor_from_array_int64_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_2d_default_layout">torch_tensor_from_array_int64_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_3d">torch_tensor_from_array_int64_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_3d_default_layout">torch_tensor_from_array_int64_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_4d">torch_tensor_from_array_int64_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_4d_default_layout">torch_tensor_from_array_int64_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_5d">torch_tensor_from_array_int64_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int64_5d_default_layout">torch_tensor_from_array_int64_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_1d">torch_tensor_from_array_int8_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_1d_default_layout">torch_tensor_from_array_int8_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_2d">torch_tensor_from_array_int8_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_2d_default_layout">torch_tensor_from_array_int8_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_3d">torch_tensor_from_array_int8_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_3d_default_layout">torch_tensor_from_array_int8_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_4d">torch_tensor_from_array_int8_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_4d_default_layout">torch_tensor_from_array_int8_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_5d">torch_tensor_from_array_int8_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_int8_5d_default_layout">torch_tensor_from_array_int8_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_1d">torch_tensor_from_array_real32_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_1d_default_layout">torch_tensor_from_array_real32_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_2d">torch_tensor_from_array_real32_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_2d_default_layout">torch_tensor_from_array_real32_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_3d">torch_tensor_from_array_real32_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_3d_default_layout">torch_tensor_from_array_real32_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_4d">torch_tensor_from_array_real32_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_4d_default_layout">torch_tensor_from_array_real32_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_5d">torch_tensor_from_array_real32_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real32_5d_default_layout">torch_tensor_from_array_real32_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_1d">torch_tensor_from_array_real64_1d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_1d_default_layout">torch_tensor_from_array_real64_1d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_2d">torch_tensor_from_array_real64_2d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_2d_default_layout">torch_tensor_from_array_real64_2d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_3d">torch_tensor_from_array_real64_3d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_3d_default_layout">torch_tensor_from_array_real64_3d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_4d">torch_tensor_from_array_real64_4d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_4d_default_layout">torch_tensor_from_array_real64_4d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_5d">torch_tensor_from_array_real64_5d</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_array_real64_5d_default_layout">torch_tensor_from_array_real64_5d_default_layout</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_from_blob">torch_tensor_from_blob</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_get_gradient">torch_tensor_get_gradient</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_mean">torch_tensor_mean</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_ones">torch_tensor_ones</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_print">torch_tensor_print</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_sum">torch_tensor_sum</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_to">torch_tensor_to</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_zero">torch_tensor_zero</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_zero_grad">torch_tensor_zero_grad</a>
            <a class="list-group-item" href="../module/ftorch.html#proc-torch_tensor_zeros">torch_tensor_zeros</a>
        </div>
      </div>
    </div>

  
  
  
  
  
  
  


  </div>

    </div>

    <div class="col-md-9" id='text'>
      <p>Main module for FTorch containing types and procedures.
 Generated from <code>ftorch.fypp</code> using the <a href="https://fypp.readthedocs.io/en/stable/index.html">fypp Fortran preprocessor</a>.</p>
<ul>
<li>License
   FTorch is released under an MIT license.
   See the <a href="https://github.com/Cambridge-ICCS/FTorch/blob/main/LICENSE">LICENSE</a>
   file for details.</li>
</ul>
<br>          <div class="card mb-4">
      <h3 class="card-header card-title bg-light">Uses</h3>
      <div class="card-body">
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
              <ul class="list-inline">
                  <li class="list-inline-item"><a href='http://fortranwiki.org/fortran/show/iso_fortran_env'>iso_fortran_env</a></li>
                  <li class="list-inline-item"><a href='http://fortranwiki.org/fortran/show/iso_c_binding'>iso_c_binding</a></li>
              </ul>
            </li>
            <li class="list-group-item">
              <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: module~~ftorch~~UsesGraph Pages: 1 -->
<svg id="moduleftorchUsesGraph" width="192pt" height="74pt"
 viewBox="0.00 0.00 192.00 74.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="module~~ftorch~~UsesGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 70)">
<title>module~~ftorch~~UsesGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-70 188,-70 188,4 -4,4"/>
<!-- module~ftorch -->
<g id="module~~ftorch~~UsesGraph_node1" class="node">
<title>module~ftorch</title>
<polygon fill="none" stroke="black" points="184,-45 130,-45 130,-21 184,-21 184,-45"/>
<text text-anchor="middle" x="157" y="-30.6" font-family="Helvetica,sans-Serif" font-size="10.50">ftorch</text>
</g>
<!-- iso_c_binding -->
<g id="module~~ftorch~~UsesGraph_node2" class="node">
<title>iso_c_binding</title>
<g id="a_module~~ftorch~~UsesGraph_node2"><a xlink:href="http://fortranwiki.org/fortran/show/iso_c_binding" xlink:title="iso_c_binding">
<polygon fill="#337ab7" stroke="#337ab7" points="89,-66 5,-66 5,-42 89,-42 89,-66"/>
<text text-anchor="middle" x="47" y="-51.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">iso_c_binding</text>
</a>
</g>
</g>
<!-- module~ftorch&#45;&gt;iso_c_binding -->
<g id="module~~ftorch~~UsesGraph_edge1" class="edge">
<title>module~ftorch&#45;&gt;iso_c_binding</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M129.86,-38.08C120.58,-39.89 109.8,-41.98 99.17,-44.05"/>
<polygon fill="#000000" stroke="#000000" points="98.22,-40.67 89.07,-46.01 99.56,-47.54 98.22,-40.67"/>
</g>
<!-- iso_fortran_env -->
<g id="module~~ftorch~~UsesGraph_node3" class="node">
<title>iso_fortran_env</title>
<g id="a_module~~ftorch~~UsesGraph_node3"><a xlink:href="http://fortranwiki.org/fortran/show/iso_fortran_env" xlink:title="iso_fortran_env">
<polygon fill="#337ab7" stroke="#337ab7" points="94,-24 0,-24 0,0 94,0 94,-24"/>
<text text-anchor="middle" x="47" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">iso_fortran_env</text>
</a>
</g>
</g>
<!-- module~ftorch&#45;&gt;iso_fortran_env -->
<g id="module~~ftorch~~UsesGraph_edge2" class="edge">
<title>module~ftorch&#45;&gt;iso_fortran_env</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M129.86,-27.92C122.05,-26.4 113.19,-24.67 104.23,-22.93"/>
<polygon fill="#000000" stroke="#000000" points="104.63,-19.45 94.14,-20.97 103.29,-26.32 104.63,-19.45"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#UsesGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="UsesGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="518pt" height="32pt"
 viewBox="0.00 0.00 517.50 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 513.5,-28 513.5,4 -4,4"/>
<!-- Module -->
<g id="node1" class="node">
<title>Module</title>
<polygon fill="#337ab7" stroke="#337ab7" points="54,-24 0,-24 0,0 54,0 54,-24"/>
<text text-anchor="middle" x="27" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Module</text>
</g>
<!-- Submodule -->
<g id="node2" class="node">
<title>Submodule</title>
<polygon fill="#5bc0de" stroke="#5bc0de" points="145.5,-24 72.5,-24 72.5,0 145.5,0 145.5,-24"/>
<text text-anchor="middle" x="109" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Submodule</text>
</g>
<!-- Subroutine -->
<g id="node3" class="node">
<title>Subroutine</title>
<polygon fill="#d9534f" stroke="#d9534f" points="234,-24 164,-24 164,0 234,0 234,-24"/>
<text text-anchor="middle" x="199" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Subroutine</text>
</g>
<!-- Function -->
<g id="node4" class="node">
<title>Function</title>
<polygon fill="#d94e8f" stroke="#d94e8f" points="310,-24 252,-24 252,0 310,0 310,-24"/>
<text text-anchor="middle" x="281" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Function</text>
</g>
<!-- Program -->
<g id="node5" class="node">
<title>Program</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="386,-24 328,-24 328,0 386,0 386,-24"/>
<text text-anchor="middle" x="357" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Program</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node6" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="509.5,-24 404.5,-24 404.5,0 509.5,0 509.5,-24"/>
<text text-anchor="middle" x="457" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a submodule to the (sub)module which it is
descended from. Dashed arrows point from a module or program unit to 
modules which it uses.
</p>
 </div>
            </div>
          </div>
        </div>
            </li>
        </ul>
      </div>
    </div>

      
      <br>


        <section>
          <h2>Variables</h2>
            <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Visibility</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
<th></th><th scope="col">Initial</th>        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-ftorch_int"></span>
              integer,
            </td>
              <td>public,</td>
            <td>
parameter               
            </td>
            <td>::</td>
            <td><strong>ftorch_int</strong></td>
<td> =</td>
                <td>int32</td>
            <td>
                
            </td>
        </tr>
    </tbody>
  </table>

        </section>
        <br>

        <section>
          <h2>Enumerations</h2>
              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="enum-__unnamed__"></span><h3>enum, bind(c)</h3></div>
    <div class="card-body">
      <h4>Enumerators</h4>
      <table class="table table-striped varlist">
        <tbody>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kUInt8</strong></td><td> = </td><td>0</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kInt8</strong></td><td> = </td><td>1</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kInt16</strong></td><td> = </td><td>2</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kInt32</strong></td><td> = </td><td>3</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kInt64</strong></td><td> = </td><td>4</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kFloat16</strong></td><td> = </td><td>5</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kFloat32</strong></td><td> = </td><td>6</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kFloat64</strong></td><td> = </td><td>7</td><td></td>
            </tr>
        </tbody>
      </table>
        <h4>Description</h4>
        
        <p>Enumerator for Torch data types
 From c_torch.h (torch_data_t)
 Note that 0 <code>torch_kUInt8</code> and 5 <code>torch_kFloat16</code> are not sypported in Fortran</p>
    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="enum-__unnamed__~2"></span><h3>enum, bind(c)</h3></div>
    <div class="card-body">
      <h4>Enumerators</h4>
      <table class="table table-striped varlist">
        <tbody>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kCPU</strong></td><td> = </td><td>0</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kCUDA</strong></td><td> = </td><td>1</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kHIP</strong></td><td> = </td><td>1</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kXPU</strong></td><td> = </td><td>11</td><td></td>
            </tr>
            <tr>
              <td>enumerator</td><td>::</td>
              <td><strong>torch_kMPS</strong></td><td> = </td><td>12</td><td></td>
            </tr>
        </tbody>
      </table>
        <h4>Description</h4>
        
        <p>Enumerator for Torch devices
 From c_torch.h (torch_device_t)
 NOTE: Defined in main CMakeLists and passed via preprocessor</p>
    </div>
  </div>

        </section>
        <br>

        <section>
          <h2>Interfaces</h2>
              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-assignment%20%28%3D%29"></span>
      <h3>public        interface <a href='../interface/assignment (=).html'>assignment (=)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_assign.html'>torch_tensor_assign</a>(output, input)  

    </h3>
    
  <p>Overloads assignment operator for tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>output</strong></td>
            <td>
<p>Tensor to assign values to</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-input"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>input</strong></td>
            <td>
<p>Tensor whose values are to be used</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-operator%20%28ASTERISK%29"></span>
      <h3>public        interface <a href='../interface/operator (ASTERISK).html'>operator (*)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_multiply.html'>torch_tensor_multiply</a>(tensor1, tensor2) result(output)  

    </h3>
    
  <p>Overloads multiplication operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor to be multiplied</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor to be multiplied</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the product</p>

          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-operator%20%28ASTERISKASTERISK%29"></span>
      <h3>public        interface <a href='../interface/operator (ASTERISKASTERISK).html'>operator (**)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_int8.html'>torch_tensor_power_int8</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~69"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_int16.html'>torch_tensor_power_int16</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~70"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~2"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_int32.html'>torch_tensor_power_int32</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~71"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~3"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_int64.html'>torch_tensor_power_int64</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~72"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~4"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_real32.html'>torch_tensor_power_real32</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~73"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~5"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Floating point exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_power_real64.html'>torch_tensor_power_real64</a>(tensor, power) result(output)  

    </h3>
    
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~74"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~6"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Floating point exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-operator%20%28%2B%29"></span>
      <h3>public        interface <a href='../interface/operator (+).html'>operator (+)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_add.html'>torch_tensor_add</a>(tensor1, tensor2) result(output)  

    </h3>
    
  <p>Overloads addition operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor to be added</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor to be added</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the sum</p>

          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-operator%20%28-%29"></span>
      <h3>public        interface <a href='../interface/operator (-).html'>operator (-)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_negative.html'>torch_tensor_negative</a>(tensor) result(output)  

    </h3>
    
  <p>Overloads negative operator for a single tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~68"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the negative of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the negative values</p>

          </li>
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_subtract.html'>torch_tensor_subtract</a>(tensor1, tensor2) result(output)  

    </h3>
    
  <p>Overloads subtraction operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor for the subtraction</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor for the subtraction</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the difference</p>

          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-operator%20%28SLASH%29"></span>
      <h3>public        interface <a href='../interface/operator (SLASH).html'>operator (/)</a> 
      </h3>
    </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  function <a href='../proc/torch_tensor_divide.html'>torch_tensor_divide</a>(tensor1, tensor2) result(output)  

    </h3>
    
  <p>Overloads division operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~4"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor for the division</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~4"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor for the division</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the quotient</p>

          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-torch_delete"></span>
      <h3>public        interface <a href='../interface/torch_delete.html'>torch_delete</a> 
      </h3>
    </div>
      <div class="card-body">
        <p>Interface for deleting generic torch objects</p>
      </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_model_delete.html'>torch_model_delete</a>(model)  

    </h3>
    
  <p>Deallocates a TorchScript model</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-model~3"></span>
              type(<a href='../type/torch_model.html'>torch_model</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>model</strong></td>
            <td>
<p>Torch Model to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_delete.html'>torch_tensor_delete</a>(tensor)  

    </h3>
    
  <p>Deallocates a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~66"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_array_delete.html'>torch_tensor_array_delete</a>(tensor_array)  

    </h3>
    
  <p>Deallocates an array of tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_array"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout),</td>
              <td></td>            <td>
              dimension(:)
            </td>
            <td>::</td>
            <td><strong>tensor_array</strong></td>
            <td>
<p>Array of tensors to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-torch_from_blob_c"></span>
      <h3>        interface
      </h3>
    </div>
    <ul class="list-group">
        <li class="list-group-item">
              <h3>
    public  function torch_from_blob_c(data, ndims, tensor_shape, strides, dtype, device_type, device_index, requires_grad) result(tensor_p) bind(c, name = 'torch_from_blob')  

    </h3>
  

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-data"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>data</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims"></span>
              integer(kind=c_int),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(*)</td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-strides"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>strides</strong>(*)</td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype"></span>
              integer(kind=c_int),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type"></span>
              integer(kind=c_int),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index"></span>
              integer(kind=c_int),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~2"></span>
              logical(kind=c_bool),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
                
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(c_ptr)</small>
    </h4>
    

        </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-torch_tensor_divide_c"></span>
      <h3>        interface
      </h3>
    </div>
    <ul class="list-group">
        <li class="list-group-item">
              <h3>
    public  subroutine torch_tensor_divide_c(output_c, tensor1_c, tensor2_c) bind(c, name = 'torch_tensor_divide')  

    </h3>
  

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output_c~2"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>output_c</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1_c~2"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>tensor1_c</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2_c~2"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>tensor2_c</strong></td>
            <td>
                
            </td>
        </tr>
    </tbody>
  </table>


        </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-torch_tensor_from_array"></span>
      <h3>public        interface <a href='../interface/torch_tensor_from_array.html'>torch_tensor_from_array</a> 
      </h3>
    </div>
      <div class="card-body">
        <p>Interface for directing <code>torch_tensor_from_array</code> to possible input types and ranks</p>
      </div>
    <ul class="list-group">
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_1d.html'>torch_tensor_from_array_int8_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~5"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~2"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~6"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~6"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~7"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_2d.html'>torch_tensor_from_array_int8_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~6"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~2"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~3"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~7"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~7"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~8"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_3d.html'>torch_tensor_from_array_int8_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~7"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~3"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~4"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~8"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~8"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~9"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_4d.html'>torch_tensor_from_array_int8_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~8"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~4"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~5"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~9"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~9"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~10"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_5d.html'>torch_tensor_from_array_int8_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~9"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~5"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~6"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~10"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~10"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~11"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_1d.html'>torch_tensor_from_array_int16_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~10"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~6"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~7"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~11"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~11"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~12"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_2d.html'>torch_tensor_from_array_int16_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~11"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~7"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~8"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~12"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~12"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~13"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_3d.html'>torch_tensor_from_array_int16_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~12"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~8"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~9"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~13"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~13"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~14"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_4d.html'>torch_tensor_from_array_int16_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~13"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~9"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~10"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~14"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~14"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~15"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_5d.html'>torch_tensor_from_array_int16_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~14"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~10"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~11"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~15"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~15"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~16"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_1d.html'>torch_tensor_from_array_int32_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~15"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~11"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~12"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~16"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~16"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~17"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_2d.html'>torch_tensor_from_array_int32_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~16"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~12"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~13"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~17"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~17"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~18"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_3d.html'>torch_tensor_from_array_int32_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~17"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~13"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~14"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~18"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~18"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~19"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_4d.html'>torch_tensor_from_array_int32_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~18"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~14"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~15"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~19"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~19"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~20"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_5d.html'>torch_tensor_from_array_int32_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~19"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~15"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~16"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~20"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~20"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~21"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_1d.html'>torch_tensor_from_array_int64_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~20"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~16"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~17"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~21"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~21"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~22"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_2d.html'>torch_tensor_from_array_int64_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~21"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~17"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~18"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~22"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~22"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~23"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_3d.html'>torch_tensor_from_array_int64_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~22"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~18"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~19"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~23"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~23"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~24"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_4d.html'>torch_tensor_from_array_int64_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~23"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~19"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~20"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~24"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~24"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~25"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_5d.html'>torch_tensor_from_array_int64_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~24"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~20"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~21"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~25"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~25"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~26"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_1d.html'>torch_tensor_from_array_real32_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~25"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~21"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~22"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~26"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~26"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~27"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_2d.html'>torch_tensor_from_array_real32_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~26"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~22"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~23"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~27"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~27"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~28"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_3d.html'>torch_tensor_from_array_real32_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~27"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~23"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~24"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~28"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~28"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~29"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_4d.html'>torch_tensor_from_array_real32_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~28"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~24"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~25"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~29"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~29"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~30"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_5d.html'>torch_tensor_from_array_real32_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~29"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~25"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~26"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~30"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~30"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~31"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_1d.html'>torch_tensor_from_array_real64_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~30"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~26"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~27"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~31"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~31"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~32"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_2d.html'>torch_tensor_from_array_real64_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~31"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~27"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~28"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~32"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~32"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~33"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_3d.html'>torch_tensor_from_array_real64_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~32"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~28"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~29"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~33"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~33"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~34"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_4d.html'>torch_tensor_from_array_real64_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~33"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~29"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~30"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~34"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~34"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~35"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_5d.html'>torch_tensor_from_array_real64_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~34"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~30"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~31"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~35"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~35"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~36"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_1d_default_layout.html'>torch_tensor_from_array_int8_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~35"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~31"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~36"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~36"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~37"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_2d_default_layout.html'>torch_tensor_from_array_int8_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~36"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~32"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~37"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~37"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~38"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_3d_default_layout.html'>torch_tensor_from_array_int8_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~37"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~33"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~38"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~38"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~39"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_4d_default_layout.html'>torch_tensor_from_array_int8_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~38"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~34"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~39"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~39"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~40"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int8_5d_default_layout.html'>torch_tensor_from_array_int8_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~39"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~35"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~40"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~40"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~41"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_1d_default_layout.html'>torch_tensor_from_array_int16_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~40"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~36"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~41"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~41"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~42"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_2d_default_layout.html'>torch_tensor_from_array_int16_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~41"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~37"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~42"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~42"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~43"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_3d_default_layout.html'>torch_tensor_from_array_int16_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~42"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~38"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~43"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~43"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~44"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_4d_default_layout.html'>torch_tensor_from_array_int16_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~43"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~39"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~44"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~44"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~45"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int16_5d_default_layout.html'>torch_tensor_from_array_int16_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~44"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~40"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~45"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~45"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~46"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_1d_default_layout.html'>torch_tensor_from_array_int32_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~45"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~41"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~46"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~46"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~47"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_2d_default_layout.html'>torch_tensor_from_array_int32_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~46"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~42"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~47"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~47"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~48"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_3d_default_layout.html'>torch_tensor_from_array_int32_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~47"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~43"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~48"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~48"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~49"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_4d_default_layout.html'>torch_tensor_from_array_int32_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~48"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~44"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~49"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~49"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~50"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int32_5d_default_layout.html'>torch_tensor_from_array_int32_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~49"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~45"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~50"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~50"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~51"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_1d_default_layout.html'>torch_tensor_from_array_int64_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~50"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~46"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~51"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~51"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~52"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_2d_default_layout.html'>torch_tensor_from_array_int64_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~51"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~47"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~52"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~52"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~53"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_3d_default_layout.html'>torch_tensor_from_array_int64_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~52"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~48"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~53"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~53"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~54"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_4d_default_layout.html'>torch_tensor_from_array_int64_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~53"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~49"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~54"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~54"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~55"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_int64_5d_default_layout.html'>torch_tensor_from_array_int64_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~54"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~50"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~55"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~55"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~56"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_1d_default_layout.html'>torch_tensor_from_array_real32_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~55"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~51"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~56"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~56"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~57"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_2d_default_layout.html'>torch_tensor_from_array_real32_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~56"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~52"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~57"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~57"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~58"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_3d_default_layout.html'>torch_tensor_from_array_real32_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~57"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~53"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~58"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~58"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~59"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_4d_default_layout.html'>torch_tensor_from_array_real32_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~58"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~54"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~59"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~59"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~60"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real32_5d_default_layout.html'>torch_tensor_from_array_real32_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~59"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~55"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~60"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~60"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~61"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_1d_default_layout.html'>torch_tensor_from_array_real64_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~60"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~56"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~61"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~61"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~62"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_2d_default_layout.html'>torch_tensor_from_array_real64_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~61"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~57"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~62"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~62"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~63"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_3d_default_layout.html'>torch_tensor_from_array_real64_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~62"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~58"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~63"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~63"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~64"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_4d_default_layout.html'>torch_tensor_from_array_real64_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~63"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~59"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~64"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~64"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~65"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
          <li class="list-group-item">
                <h3>
    public  subroutine <a href='../proc/torch_tensor_from_array_real64_5d_default_layout.html'>torch_tensor_from_array_real64_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  

    </h3>
    
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~64"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~60"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~65"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~65"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~66"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


          </li>
    </ul>
  </div>

              <div class="card">
    <div class="card-header bg-light codesum">
      <span class="anchor" id="interface-torch_tensor_multiply_c"></span>
      <h3>        interface
      </h3>
    </div>
    <ul class="list-group">
        <li class="list-group-item">
              <h3>
    public  subroutine torch_tensor_multiply_c(output_c, tensor1_c, tensor2_c) bind(c, name = 'torch_tensor_multiply')  

    </h3>
  

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output_c"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>output_c</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1_c"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>tensor1_c</strong></td>
            <td>
                
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2_c"></span>
              type(c_ptr),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              value
            </td>
            <td>::</td>
            <td><strong>tensor2_c</strong></td>
            <td>
                
            </td>
        </tr>
    </tbody>
  </table>


        </li>
    </ul>
  </div>

        </section>
        <br>


        <section>
          <h2>Derived Types</h2>
              <div class="card">
    <div class="card-header codesum">
      <span class="anchor" id="type-torch_model"></span>
      <h3>
        type, public&nbsp;::&nbsp;
        <a href='../type/torch_model.html'>torch_model</a>
        
      </h3>
    </div>
    <div class="card-body">
      <p>Type for holding a torch neural net (nn.Module).</p>

        <h4>Components</h4>
          <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Visibility</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
<th></th><th scope="col">Initial</th>        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-p"></span>
              type(c_ptr),
            </td>
              <td>public</td>
            <td>
              
            </td>
            <td>::</td>
            <td><strong>p</strong></td>
<td> =</td>
                <td>c_null_ptr</td>
            <td>
<p>pointer to the neural net in memory</p>
            </td>
        </tr>
    </tbody>
  </table>




    </div>
  </div>

              <div class="card">
    <div class="card-header codesum">
      <span class="anchor" id="type-torch_tensor"></span>
      <h3>
        type, public&nbsp;::&nbsp;
        <a href='../type/torch_tensor.html'>torch_tensor</a>
        
      </h3>
    </div>
    <div class="card-body">
      <p>Type for holding a Torch tensor.</p>

        <h4>Components</h4>
          <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Visibility</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
<th></th><th scope="col">Initial</th>        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-p~2"></span>
              type(c_ptr),
            </td>
              <td>public</td>
            <td>
              
            </td>
            <td>::</td>
            <td><strong>p</strong></td>
<td> =</td>
                <td>c_null_ptr</td>
            <td>
<p>pointer to the tensor in memory</p>
            </td>
        </tr>
    </tbody>
  </table>



        <h4>Finalizations Procedures</h4>
        <table class="table table-striped varlist">
          <tbody>
              <tr>
                <td>final :: <strong>torch_tensor_delete</strong></td>
                <td></td>
              </tr>
          </tbody>
        </table>

        <h4>Type-Bound Procedures</h4>
        <table class="table table-striped varlist">
          <tbody>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-get_device_index'>get_device_index</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_get_device_index.html'>torch_tensor_get_device_index</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-get_device_type'>get_device_type</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_get_device_type.html'>torch_tensor_get_device_type</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-get_dtype'>get_dtype</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_get_dtype.html'>torch_tensor_get_dtype</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-get_rank'>get_rank</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_get_rank.html'>torch_tensor_get_rank</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-get_shape'>get_shape</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_get_shape.html'>torch_tensor_get_shape</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-requires_grad'>requires_grad</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_requires_grad.html'>torch_tensor_requires_grad</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-zero'>zero</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_zero.html'>torch_tensor_zero</a></td>
                  <td></td>
              </tr>
              <tr>
                <td>  procedure, public ::
  <strong><a href='../type/torch_tensor.html#boundprocedure-zero_grad'>zero_grad</a></strong> => <a href='/home/runner/work/FTorch/FTorch/doc/proc/torch_tensor_zero_grad.html'>torch_tensor_zero_grad</a></td>
                  <td></td>
              </tr>
          </tbody>
        </table>
    </div>
  </div>

        </section>
        <br>

        <section>
          <h2>Functions</h2>
              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_add"></span> <h3>public  function <a href='../proc/torch_tensor_add.html'>torch_tensor_add</a>(tensor1, tensor2) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads addition operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor to be added</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor to be added</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the sum</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_divide"></span> <h3>public  function <a href='../proc/torch_tensor_divide.html'>torch_tensor_divide</a>(tensor1, tensor2) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads division operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~4"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor for the division</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~4"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor for the division</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the quotient</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_device_index"></span> <h3>public  function <a href='../proc/torch_tensor_get_device_index.html'>torch_tensor_get_device_index</a>(self) result(device_index)  
</h3></div>
    <div class="card-body">
          
  <p>Determines the device index of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self~5"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to get the device index of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>integer(kind=c_int)</small>
    </h4>
    <p>Device index of tensor</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_device_type"></span> <h3>public  function <a href='../proc/torch_tensor_get_device_type.html'>torch_tensor_get_device_type</a>(self) result(device_type)  
</h3></div>
    <div class="card-body">
          
  <p>Returns the device type of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self~4"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to get the device type of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>integer(kind=c_int)</small>
    </h4>
    <p>Device type of tensor</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_dtype"></span> <h3>public  function <a href='../proc/torch_tensor_get_dtype.html'>torch_tensor_get_dtype</a>(self) result(dtype)  
</h3></div>
    <div class="card-body">
          
  <p>Returns the data type of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self~3"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to get the data type of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>integer(kind=c_int)</small>
    </h4>
    <p>Data type of tensor</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_rank"></span> <h3>public  function <a href='../proc/torch_tensor_get_rank.html'>torch_tensor_get_rank</a>(self) result(rank)  
</h3></div>
    <div class="card-body">
          
  <p>Determines the rank of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to get the rank of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>integer(kind=int32)</small>
    </h4>
    <p>Rank of tensor</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_shape"></span> <h3>public  function <a href='../proc/torch_tensor_get_shape.html'>torch_tensor_get_shape</a>(self) result(sizes)  
</h3></div>
    <div class="card-body">
          
  <p>Determines the shape of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self~2"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to get the shape of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>integer(kind=c_long), pointer, (:)</small>
    </h4>
    <p>Pointer to tensor data</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_multiply"></span> <h3>public  function <a href='../proc/torch_tensor_multiply.html'>torch_tensor_multiply</a>(tensor1, tensor2) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads multiplication operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor to be multiplied</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor to be multiplied</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the product</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_negative"></span> <h3>public  function <a href='../proc/torch_tensor_negative.html'>torch_tensor_negative</a>(tensor) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads negative operator for a single tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~68"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the negative of</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the negative values</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_int16"></span> <h3>public  function <a href='../proc/torch_tensor_power_int16.html'>torch_tensor_power_int16</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~70"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~2"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_int32"></span> <h3>public  function <a href='../proc/torch_tensor_power_int32.html'>torch_tensor_power_int32</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~71"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~3"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_int64"></span> <h3>public  function <a href='../proc/torch_tensor_power_int64.html'>torch_tensor_power_int64</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~72"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~4"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_int8"></span> <h3>public  function <a href='../proc/torch_tensor_power_int8.html'>torch_tensor_power_int8</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~69"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Integer exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_real32"></span> <h3>public  function <a href='../proc/torch_tensor_power_real32.html'>torch_tensor_power_real32</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~73"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~5"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Floating point exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_power_real64"></span> <h3>public  function <a href='../proc/torch_tensor_power_real64.html'>torch_tensor_power_real64</a>(tensor, power) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads exponentiation operator for a tensor and a scalar of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~74"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to take the power of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-power~6"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>power</strong></td>
            <td>
<p>Floating point exponent</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the exponentiation</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_requires_grad"></span> <h3>public  function <a href='../proc/torch_tensor_requires_grad.html'>torch_tensor_requires_grad</a>(self) result(requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Determines whether a tensor requires the autograd module.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-self~6"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>self</strong></td>
            <td>
<p>Tensor to query</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>logical</small>
    </h4>
    <p>Whether the tensor requires autograd</p>

    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_subtract"></span> <h3>public  function <a href='../proc/torch_tensor_subtract.html'>torch_tensor_subtract</a>(tensor1, tensor2) result(output)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads subtraction operator for two tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor1~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor1</strong></td>
            <td>
<p>First tensor for the subtraction</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor2~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor2</strong></td>
            <td>
<p>Second tensor for the subtraction</p>
            </td>
        </tr>
    </tbody>
  </table>

    <h4>
    Return Value
    <small>type(<a href='../type/torch_tensor.html'>torch_tensor</a>)</small>
    </h4>
    <p>Tensor to hold the difference</p>

    </div>
  </div>

        </section>
        <br>

        <section>
          <h2>Subroutines</h2>
              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_model_delete"></span> <h3>public  subroutine <a href='../proc/torch_model_delete.html'>torch_model_delete</a>(model)  
</h3></div>
    <div class="card-body">
          
  <p>Deallocates a TorchScript model</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-model~3"></span>
              type(<a href='../type/torch_model.html'>torch_model</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>model</strong></td>
            <td>
<p>Torch Model to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_model_forward"></span> <h3>public  subroutine <a href='../proc/torch_model_forward.html'>torch_model_forward</a>(model, input_tensors, output_tensors, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Performs a forward pass of the model with the input tensors</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-model~2"></span>
              type(<a href='../type/torch_model.html'>torch_model</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>model</strong></td>
            <td>
<p>Model</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-input_tensors"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              dimension(:)
            </td>
            <td>::</td>
            <td><strong>input_tensors</strong></td>
            <td>
<p>Array of Input tensors</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-output_tensors"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              dimension(:)
            </td>
            <td>::</td>
            <td><strong>output_tensors</strong></td>
            <td>
<p>Returned output tensors</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~69"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_model_load"></span> <h3>public  subroutine <a href='../proc/torch_model_load.html'>torch_model_load</a>(model, filename, device_type, device_index, requires_grad, is_training)  
</h3></div>
    <div class="card-body">
          
  <p>Loads a TorchScript nn.module (pre-trained PyTorch model saved with TorchScript)</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-model"></span>
              type(<a href='../type/torch_model.html'>torch_model</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>model</strong></td>
            <td>
<p>Returned deserialized model</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-filename"></span>
              character(len=*),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>filename</strong></td>
            <td>
<p>Filename of saved TorchScript model</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~67"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~67"></span>
              integer(kind=c_int),
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~68"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-is_training"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>is_training</strong></td>
            <td>
<p>Whether the model is being trained, rather than evaluated</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_array_delete"></span> <h3>public  subroutine <a href='../proc/torch_tensor_array_delete.html'>torch_tensor_array_delete</a>(tensor_array)  
</h3></div>
    <div class="card-body">
          
  <p>Deallocates an array of tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_array"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout),</td>
              <td></td>            <td>
              dimension(:)
            </td>
            <td>::</td>
            <td><strong>tensor_array</strong></td>
            <td>
<p>Array of tensors to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_assign"></span> <h3>public  subroutine <a href='../proc/torch_tensor_assign.html'>torch_tensor_assign</a>(output, input)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads assignment operator for tensors.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>output</strong></td>
            <td>
<p>Tensor to assign values to</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-input"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>input</strong></td>
            <td>
<p>Tensor whose values are to be used</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_backward"></span> <h3>public  subroutine <a href='../proc/torch_tensor_backward.html'>torch_tensor_backward</a>(tensor, retain_graph)  
</h3></div>
    <div class="card-body">
          
  <p>Performs back-propagation on a Torch Tensor, given some external gradient.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~78"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to compute gradients of</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-retain_graph"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>retain_graph</strong></td>
            <td>
<p>Should the computational graph be retained?</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_delete"></span> <h3>public  subroutine <a href='../proc/torch_tensor_delete.html'>torch_tensor_delete</a>(tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Deallocates a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~66"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to deallocate</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_empty"></span> <h3>public  subroutine <a href='../proc/torch_tensor_empty.html'>torch_tensor_empty</a>(tensor, ndims, tensor_shape, dtype, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Returns a tensor with uninitialised values.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~2"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
<p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~2"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
<p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~2"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
<p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~2"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~2"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~3"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_1d.html'>torch_tensor_from_array_int16_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~10"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~6"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~7"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~11"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~11"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~12"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_1d_default_layout.html'>torch_tensor_from_array_int16_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~40"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~36"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~41"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~41"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~42"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_2d.html'>torch_tensor_from_array_int16_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~11"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~7"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~8"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~12"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~12"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~13"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_2d_default_layout.html'>torch_tensor_from_array_int16_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~41"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~37"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~42"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~42"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~43"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_3d.html'>torch_tensor_from_array_int16_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~12"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~8"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~9"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~13"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~13"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~14"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_3d_default_layout.html'>torch_tensor_from_array_int16_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~42"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~38"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~43"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~43"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~44"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_4d.html'>torch_tensor_from_array_int16_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~13"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~9"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~10"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~14"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~14"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~15"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_4d_default_layout.html'>torch_tensor_from_array_int16_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~43"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~39"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~44"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~44"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~45"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_5d.html'>torch_tensor_from_array_int16_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int16</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~14"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~10"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~11"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~15"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~15"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~16"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int16_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int16_5d_default_layout.html'>torch_tensor_from_array_int16_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int16</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~44"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~40"></span>
              integer(kind=int16),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~45"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~45"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~46"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_1d.html'>torch_tensor_from_array_int32_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~15"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~11"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~12"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~16"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~16"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~17"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_1d_default_layout.html'>torch_tensor_from_array_int32_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~45"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~41"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~46"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~46"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~47"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_2d.html'>torch_tensor_from_array_int32_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~16"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~12"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~13"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~17"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~17"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~18"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_2d_default_layout.html'>torch_tensor_from_array_int32_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~46"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~42"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~47"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~47"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~48"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_3d.html'>torch_tensor_from_array_int32_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~17"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~13"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~14"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~18"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~18"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~19"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_3d_default_layout.html'>torch_tensor_from_array_int32_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~47"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~43"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~48"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~48"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~49"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_4d.html'>torch_tensor_from_array_int32_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~18"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~14"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~15"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~19"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~19"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~20"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_4d_default_layout.html'>torch_tensor_from_array_int32_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~48"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~44"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~49"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~49"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~50"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_5d.html'>torch_tensor_from_array_int32_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~19"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~15"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~16"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~20"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~20"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~21"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int32_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int32_5d_default_layout.html'>torch_tensor_from_array_int32_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~49"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~45"></span>
              integer(kind=int32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~50"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~50"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~51"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_1d.html'>torch_tensor_from_array_int64_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~20"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~16"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~17"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~21"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~21"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~22"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_1d_default_layout.html'>torch_tensor_from_array_int64_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~50"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~46"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~51"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~51"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~52"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_2d.html'>torch_tensor_from_array_int64_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~21"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~17"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~18"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~22"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~22"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~23"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_2d_default_layout.html'>torch_tensor_from_array_int64_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~51"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~47"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~52"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~52"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~53"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_3d.html'>torch_tensor_from_array_int64_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~22"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~18"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~19"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~23"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~23"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~24"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_3d_default_layout.html'>torch_tensor_from_array_int64_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~52"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~48"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~53"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~53"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~54"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_4d.html'>torch_tensor_from_array_int64_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~23"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~19"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~20"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~24"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~24"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~25"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_4d_default_layout.html'>torch_tensor_from_array_int64_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~53"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~49"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~54"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~54"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~55"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_5d.html'>torch_tensor_from_array_int64_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~24"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~20"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~21"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~25"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~25"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~26"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int64_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int64_5d_default_layout.html'>torch_tensor_from_array_int64_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~54"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~50"></span>
              integer(kind=int64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~55"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~55"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~56"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_1d.html'>torch_tensor_from_array_int8_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~5"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~2"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~6"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~6"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~7"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_1d_default_layout.html'>torch_tensor_from_array_int8_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~35"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~31"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~36"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~36"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~37"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_2d.html'>torch_tensor_from_array_int8_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~6"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~2"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~3"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~7"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~7"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~8"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_2d_default_layout.html'>torch_tensor_from_array_int8_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~36"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~32"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~37"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~37"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~38"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_3d.html'>torch_tensor_from_array_int8_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~7"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~3"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~4"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~8"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~8"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~9"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_3d_default_layout.html'>torch_tensor_from_array_int8_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~37"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~33"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~38"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~38"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~39"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_4d.html'>torch_tensor_from_array_int8_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~8"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~4"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~5"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~9"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~9"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~10"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_4d_default_layout.html'>torch_tensor_from_array_int8_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~38"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~34"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~39"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~39"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~40"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_5d.html'>torch_tensor_from_array_int8_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int8</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~9"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~5"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~6"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~10"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~10"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~11"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_int8_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_int8_5d_default_layout.html'>torch_tensor_from_array_int8_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>int8</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~39"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~35"></span>
              integer(kind=int8),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~40"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~40"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~41"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_1d.html'>torch_tensor_from_array_real32_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~25"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~21"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~22"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~26"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~26"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~27"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_1d_default_layout.html'>torch_tensor_from_array_real32_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~55"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~51"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~56"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~56"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~57"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_2d.html'>torch_tensor_from_array_real32_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~26"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~22"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~23"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~27"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~27"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~28"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_2d_default_layout.html'>torch_tensor_from_array_real32_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~56"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~52"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~57"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~57"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~58"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_3d.html'>torch_tensor_from_array_real32_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~27"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~23"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~24"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~28"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~28"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~29"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_3d_default_layout.html'>torch_tensor_from_array_real32_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~57"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~53"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~58"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~58"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~59"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_4d.html'>torch_tensor_from_array_real32_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~28"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~24"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~25"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~29"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~29"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~30"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_4d_default_layout.html'>torch_tensor_from_array_real32_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~58"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~54"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~59"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~59"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~60"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_5d.html'>torch_tensor_from_array_real32_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real32</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~29"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~25"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~26"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~30"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~30"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~31"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real32_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real32_5d_default_layout.html'>torch_tensor_from_array_real32_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real32</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~59"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~55"></span>
              real(kind=real32),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~60"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~60"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~61"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_1d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_1d.html'>torch_tensor_from_array_real64_1d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~30"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~26"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~27"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(1)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~31"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~31"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~32"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_1d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_1d_default_layout.html'>torch_tensor_from_array_real64_1d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 1 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~60"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~56"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~61"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~61"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~62"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_2d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_2d.html'>torch_tensor_from_array_real64_2d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~31"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~27"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~28"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(2)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~32"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~32"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~33"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_2d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_2d_default_layout.html'>torch_tensor_from_array_real64_2d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 2 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~61"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~57"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~62"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~62"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~63"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_3d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_3d.html'>torch_tensor_from_array_real64_3d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~32"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~28"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~29"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(3)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~33"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~33"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~34"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_3d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_3d_default_layout.html'>torch_tensor_from_array_real64_3d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 3 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~62"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~58"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~63"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~63"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~64"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_4d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_4d.html'>torch_tensor_from_array_real64_4d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~33"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~29"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~30"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(4)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~34"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~34"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~35"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_4d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_4d_default_layout.html'>torch_tensor_from_array_real64_4d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 4 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~63"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~59"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~64"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~64"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~65"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_5d"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_5d.html'>torch_tensor_from_array_real64_5d</a>(tensor, data_in, layout, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real64</code></p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~34"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~30"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout~31"></span>
              integer(kind=ftorch_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(5)</td>
            <td>
<p>Control order of indices</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~35"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~35"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~36"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_array_real64_5d_default_layout"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_array_real64_5d_default_layout.html'>torch_tensor_from_array_real64_5d_default_layout</a>(tensor, data_in, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Return a Torch tensor pointing to data_in array of rank 5 containing data of type <code>real64</code> with default layout [1, 2, ..., n].</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~64"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data_in~60"></span>
              real(kind=real64),
            </td>
<td>intent(in),</td>
              <td></td>            <td>
              target
            </td>
            <td>::</td>
            <td><strong>data_in</strong>(:,:,:,:,:)</td>
            <td>
<p>Input data that tensor will point at</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~65"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~65"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~66"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_from_blob"></span> <h3>public  subroutine <a href='../proc/torch_tensor_from_blob.html'>torch_tensor_from_blob</a>(tensor, data, ndims, tensor_shape, layout, dtype, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Exposes the given data as a tensor without taking ownership of the original data.
 This routine will take an (i, j, k) array and return an (k, j, i) tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~4"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-data~2"></span>
              type(c_ptr),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>data</strong></td>
            <td>
<p>Pointer to data</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~5"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
<p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~5"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
<p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-layout"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>layout</strong>(:)</td>
            <td>
<p>Layout for strides for accessing data</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~5"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
<p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~5"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~5"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~6"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_get_gradient"></span> <h3>public  subroutine <a href='../proc/torch_tensor_get_gradient.html'>torch_tensor_get_gradient</a>(gradient, tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Retrieves the gradient with respect to a Torch Tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-gradient"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>gradient</strong></td>
            <td>
<p>Tensor holding the gradient</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~79"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to compute the gradient with respect to</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_mean"></span> <h3>public  subroutine <a href='../proc/torch_tensor_mean.html'>torch_tensor_mean</a>(output, tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads mean operator over the values in a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output~14"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>output</strong></td>
            <td>
<p>Tensor holding the averaged values</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~76"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to average the values of</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_ones"></span> <h3>public  subroutine <a href='../proc/torch_tensor_ones.html'>torch_tensor_ones</a>(tensor, ndims, tensor_shape, dtype, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Returns a tensor filled with the scalar value 1.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~3"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
<p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~4"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
<p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
<p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~4"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~4"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~5"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_print"></span> <h3>public  subroutine <a href='../proc/torch_tensor_print.html'>torch_tensor_print</a>(tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Prints the contents of a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~65"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to print the contents of</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_sum"></span> <h3>public  subroutine <a href='../proc/torch_tensor_sum.html'>torch_tensor_sum</a>(output, tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Overloads summation operator over the values in a tensor.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-output~13"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>output</strong></td>
            <td>
<p>Tensor holding the summed values</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~75"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to sum the values of</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_to"></span> <h3>public  subroutine <a href='../proc/torch_tensor_to.html'>torch_tensor_to</a>(source_tensor, target_tensor, non_blocking)  
</h3></div>
    <div class="card-body">
          
  <p>Moves a source_tensor tensor to a target tensor's device and dtype</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-source_tensor"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>source_tensor</strong></td>
            <td>
<p>Source tensor to be moved</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-target_tensor"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>target_tensor</strong></td>
            <td>
<p>Target tensor with the desired device and dtype</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-non_blocking"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>non_blocking</strong></td>
            <td>
<p>Whether to perform asynchronous copy</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_zero"></span> <h3>public  subroutine <a href='../proc/torch_tensor_zero.html'>torch_tensor_zero</a>(tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Fills a tensor with the scalar value 0.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~67"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor whose values are to be zeroed</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_zero_grad"></span> <h3>public  subroutine <a href='../proc/torch_tensor_zero_grad.html'>torch_tensor_zero_grad</a>(tensor)  
</h3></div>
    <div class="card-body">
          
  <p>Resets a tensor's gradient to zero.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~77"></span>
              class(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(inout)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Tensor to zero the gradient of</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

              <div class="card">
    <div class="card-header codesum"><span class="anchor" id="proc-torch_tensor_zeros"></span> <h3>public  subroutine <a href='../proc/torch_tensor_zeros.html'>torch_tensor_zeros</a>(tensor, ndims, tensor_shape, dtype, device_type, device_index, requires_grad)  
</h3></div>
    <div class="card-body">
          
  <p>Returns a tensor filled with the scalar value 0.</p>

  <h4>Arguments</h4>
      <table class="table table-striped varlist">
    <thead>
      <tr>
        <th scope="col">Type</th>
<th scope="col">Intent</th><th scope="col">Optional</th>        <th scope="col">Attributes</th>
        <th scope="col"></th>
        <th scope="col">Name</th>
        <th scope="col"></th>
    </thead>
    <tbody>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor~2"></span>
              type(<a href='../type/torch_tensor.html'>torch_tensor</a>),
            </td>
<td>intent(out)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor</strong></td>
            <td>
<p>Returned tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-ndims~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>ndims</strong></td>
            <td>
<p>Number of dimensions of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-tensor_shape~3"></span>
              integer(kind=c_int64_t),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>tensor_shape</strong>(:)</td>
            <td>
<p>Shape of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-dtype~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>dtype</strong></td>
            <td>
<p>Data type of the tensor</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_type~3"></span>
              integer(kind=c_int),
            </td>
<td>intent(in)</td>
              <td></td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_type</strong></td>
            <td>
<p>Device type the tensor will live on (<code>torch_kCPU</code> or <code>torch_kCUDA</code>)</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-device_index~3"></span>
              integer,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>device_index</strong></td>
            <td>
<p>Device index to use for <code>torch_kCUDA</code> case</p>
            </td>
        </tr>
        <tr>
            <td>
              <span class="anchor" id="variable-requires_grad~4"></span>
              logical,
            </td>
<td>intent(in),</td>
              <td>optional</td>            <td>
              
            </td>
            <td>::</td>
            <td><strong>requires_grad</strong></td>
            <td>
<p>Whether gradients need to be computed for the created tensor</p>
            </td>
        </tr>
    </tbody>
  </table>


    </div>
  </div>

        </section>
        <br>



    </div>
  </div>

      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2025 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>