<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="FTorch - A library for coupling (Py)Torch machine learning models to Fortran codes.Written in modern Fortran (2008) with source code available on GitHub it has been used in multiple scientific projects.The associated JOSS paper can read here.">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>ftorch_model.f90 &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
      <link href="../css/user.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="../page/index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>ftorch_model.f90
      <small>Source File</small>
      
    </h1>
      <div class="container p-2 mb-4 bg-light border rounded-3">
    <div class="row align-items-center justify-content-between" id="info-bar">
      <div class="col">
        <ul class="list-inline" style="margin-bottom:0px;display:inline">
            <li class="list-inline-item" id="meta-license"><i class="fa fa-legal"></i> <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a></li>

            <li class="list-inline-item" id="statements"><i class="fa fa-list-ol"></i>
              <a data-bs-toggle="tooltip" data-bs-placement="bottom" data-bs-html="true"
                 title=" 5.9% of total for source files.">136 statements</a>
            </li>

            <li class="list-inline-item" id="source-file">
              <i class="fa fa-code"></i>
              <a href="../src/ftorch_model.f90"> Source File</a>
            </li>
        </ul>
      </div>
      <div class="col">
        <nav aria-label="breadcrumb">
          <ol class="breadcrumb justify-content-end mb-0">
            <li class="breadcrumb-item active" aria-current="page">ftorch_model.f90</li>
          </ol>
        </nav>
      </div>
    </div>
  </div>
  <script>
    // Enable Bootstrap tooltips
    (function () {
      const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
      const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
    })();
  </script>

  </div>
  <div class="row">
    <div class="col-md-3 hidden-xs hidden-sm visible-md visible-lg">
        <div id="sidebar">
      <h3>Contents</h3>
  
  
  
  
  
      <div class="card mb-4">
      <a data-bs-toggle="collapse" href="#mods-0"
         aria-expanded="false" aria-controls="mods-0">
         <h4 class="card-header bg-primary text-white">Modules</h4>
      </a>
      <div id="mods-0" class="collapse">
        <div class="list-group list-group-flush">
            <a class="list-group-item" href="../module/ftorch_model.html">ftorch_model</a>
        </div>
      </div>
    </div>

  
  
  
  
  
  
  
  
  
  
  
  
  
  
    <div class="card card-primary">
      <div class="card-header text-left"><h3 class="card-title">Source Code</h3></div>
      <div class="list-group">
        <a class="list-group-item" href="../sourcefile/ftorch_model.f90.html#src">ftorch_model.f90</a>
      </div>
    </div>


  </div>

    </div>
    <div class="col-md-9" id='text'>
      
      <br>
        <div class="card">
          <div class="card-header">
            <h3 class="card-title">This file depends on</h3>
          </div>
          <div class="card-body">
            <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: sourcefile~~ftorch_model.f90~~EfferentGraph Pages: 1 -->
<svg id="sourcefileftorch_modelf90EfferentGraph" width="396pt" height="83pt"
 viewBox="0.00 0.00 396.00 83.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 79)">
<title>sourcefile~~ftorch_model.f90~~EfferentGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-79 392,-79 392,4 -4,4"/>
<!-- sourcefile~ftorch_model.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_node1" class="node">
<title>sourcefile~ftorch_model.f90</title>
<polygon fill="none" stroke="black" points="388,-45 285,-45 285,-21 388,-21 388,-45"/>
<text text-anchor="middle" x="336.5" y="-30.6" font-family="Helvetica,sans-Serif" font-size="10.50">ftorch_model.f90</text>
</g>
<!-- sourcefile~ftorch_devices.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_node2" class="node">
<title>sourcefile~ftorch_devices.f90</title>
<g id="a_sourcefile~~ftorch_model.f90~~EfferentGraph_node2"><a xlink:href="../sourcefile/ftorch_devices.f90.html" xlink:title="ftorch_devices.F90">
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="112,-75 0,-75 0,-51 112,-51 112,-75"/>
<text text-anchor="middle" x="56" y="-60.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">ftorch_devices.F90</text>
</a>
</g>
</g>
<!-- sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_devices.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_edge1" class="edge">
<title>sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_devices.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M292.7,-45.11C278.86,-48.56 263.38,-51.94 249,-54 207.18,-60 159.77,-62.27 122.52,-63.03"/>
<polygon fill="#000000" stroke="#000000" points="122.24,-59.53 112.3,-63.2 122.36,-66.53 122.24,-59.53"/>
</g>
<!-- sourcefile~ftorch_tensor.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_node3" class="node">
<title>sourcefile~ftorch_tensor.f90</title>
<g id="a_sourcefile~~ftorch_model.f90~~EfferentGraph_node3"><a xlink:href="../sourcefile/ftorch_tensor.f90.html" xlink:title="ftorch_tensor.f90">
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="249,-45 148,-45 148,-21 249,-21 249,-45"/>
<text text-anchor="middle" x="198.5" y="-30.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">ftorch_tensor.f90</text>
</a>
</g>
</g>
<!-- sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_tensor.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_edge2" class="edge">
<title>sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_tensor.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M284.86,-33C276.59,-33 267.95,-33 259.49,-33"/>
<polygon fill="#000000" stroke="#000000" points="259.28,-29.5 249.28,-33 259.28,-36.5 259.28,-29.5"/>
</g>
<!-- sourcefile~ftorch_types.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_node4" class="node">
<title>sourcefile~ftorch_types.f90</title>
<g id="a_sourcefile~~ftorch_model.f90~~EfferentGraph_node4"><a xlink:href="../sourcefile/ftorch_types.f90.html" xlink:title="ftorch_types.f90">
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="104.5,-24 7.5,-24 7.5,0 104.5,0 104.5,-24"/>
<text text-anchor="middle" x="56" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">ftorch_types.f90</text>
</a>
</g>
</g>
<!-- sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_types.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_edge3" class="edge">
<title>sourcefile~ftorch_model.f90&#45;&gt;sourcefile~ftorch_types.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M294.51,-20.93C280.24,-17.3 264.04,-13.8 249,-12 204.23,-6.64 153.16,-6.95 114.95,-8.43"/>
<polygon fill="#000000" stroke="#000000" points="114.37,-4.95 104.53,-8.87 114.67,-11.94 114.37,-4.95"/>
</g>
<!-- sourcefile~ftorch_tensor.f90&#45;&gt;sourcefile~ftorch_devices.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_edge4" class="edge">
<title>sourcefile~ftorch_tensor.f90&#45;&gt;sourcefile~ftorch_devices.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M147.96,-43.58C139.67,-45.35 130.96,-47.21 122.35,-49.05"/>
<polygon fill="#000000" stroke="#000000" points="121.36,-45.68 112.31,-51.19 122.82,-52.52 121.36,-45.68"/>
</g>
<!-- sourcefile~ftorch_tensor.f90&#45;&gt;sourcefile~ftorch_types.f90 -->
<g id="sourcefile~~ftorch_model.f90~~EfferentGraph_edge5" class="edge">
<title>sourcefile~ftorch_tensor.f90&#45;&gt;sourcefile~ftorch_types.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M147.96,-25.6C137.2,-23.99 125.72,-22.27 114.67,-20.62"/>
<polygon fill="#000000" stroke="#000000" points="115.08,-17.14 104.67,-19.13 114.05,-24.07 115.08,-17.14"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#EfferentGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="EfferentGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="202pt" height="32pt"
 viewBox="0.00 0.00 201.50 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 197.5,-28 197.5,4 -4,4"/>
<!-- Source File -->
<g id="node1" class="node">
<title>Source File</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="70,-24 0,-24 0,0 70,0 70,-24"/>
<text text-anchor="middle" x="35" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Source File</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node2" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="193.5,-24 88.5,-24 88.5,0 193.5,0 193.5,-24"/>
<text text-anchor="middle" x="141" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a file to a file which it depends on. A file
is dependent upon another if the latter must be compiled before the former
can be.
</p>
 </div>
            </div>
          </div>
        </div>
          </div>
        </div>
        <div class="card">
          <div class="card-header">
            <h3 class="card-title">Files dependent on this one</h3>
          </div>
          <div class="card-body">
            <div class="depgraph"><?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: sourcefile~~ftorch_model.f90~~AfferentGraph Pages: 1 -->
<svg id="sourcefileftorch_modelf90AfferentGraph" width="211pt" height="32pt"
 viewBox="0.00 0.00 211.00 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="sourcefile~~ftorch_model.f90~~AfferentGraph" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>sourcefile~~ftorch_model.f90~~AfferentGraph</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 207,-28 207,4 -4,4"/>
<!-- sourcefile~ftorch_model.f90 -->
<g id="sourcefile~~ftorch_model.f90~~AfferentGraph_node1" class="node">
<title>sourcefile~ftorch_model.f90</title>
<polygon fill="none" stroke="black" points="103,-24 0,-24 0,0 103,0 103,-24"/>
<text text-anchor="middle" x="51.5" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">ftorch_model.f90</text>
</g>
<!-- sourcefile~ftorch.f90 -->
<g id="sourcefile~~ftorch_model.f90~~AfferentGraph_node2" class="node">
<title>sourcefile~ftorch.f90</title>
<g id="a_sourcefile~~ftorch_model.f90~~AfferentGraph_node2"><a xlink:href="../sourcefile/ftorch.f90.html" xlink:title="ftorch.f90">
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="203,-24 139,-24 139,0 203,0 203,-24"/>
<text text-anchor="middle" x="171" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">ftorch.f90</text>
</a>
</g>
</g>
<!-- sourcefile~ftorch.f90&#45;&gt;sourcefile~ftorch_model.f90 -->
<g id="sourcefile~~ftorch_model.f90~~AfferentGraph_edge1" class="edge">
<title>sourcefile~ftorch.f90&#45;&gt;sourcefile~ftorch_model.f90</title>
<path fill="none" stroke="#000000" stroke-dasharray="5,2" d="M138.82,-12C131,-12 122.34,-12 113.61,-12"/>
<polygon fill="#000000" stroke="#000000" points="113.38,-8.5 103.38,-12 113.38,-15.5 113.38,-8.5"/>
</g>
</g>
</svg>
</div>          <div>
            <a type="button" class="graph-help" data-bs-toggle="modal" href="#AfferentGraph-help-text">Help</a>
          </div>
          <div class="modal fade" id="AfferentGraph-help-text" tabindex="-1" role="dialog">
            <div class="modal-dialog modal-lg" role="document">
              <div class="modal-content">
                <div class="modal-header">
                  <h4 class="modal-title" id="-graph-help-label">Graph Key</h4>
                  <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
<p>Nodes of different colours represent the following: </p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: Graph Key Pages: 1 -->
<svg width="202pt" height="32pt"
 viewBox="0.00 0.00 201.50 32.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 28)">
<title>Graph Key</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-28 197.5,-28 197.5,4 -4,4"/>
<!-- Source File -->
<g id="node1" class="node">
<title>Source File</title>
<polygon fill="#f0ad4e" stroke="#f0ad4e" points="70,-24 0,-24 0,0 70,0 70,-24"/>
<text text-anchor="middle" x="35" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50" fill="white">Source File</text>
</g>
<!-- This Page&#39;s Entity -->
<g id="node2" class="node">
<title>This Page&#39;s Entity</title>
<polygon fill="none" stroke="black" points="193.5,-24 88.5,-24 88.5,0 193.5,0 193.5,-24"/>
<text text-anchor="middle" x="141" y="-9.6" font-family="Helvetica,sans-Serif" font-size="10.50">This Page&#39;s Entity</text>
</g>
</g>
</svg>

<p>Solid arrows point from a file to a file which it depends on. A file
is dependent upon another if the latter must be compiled before the former
can be.
</p>
 </div>
            </div>
          </div>
        </div>
          </div>
        </div>
<br>
      <section>
        <h2><span class="anchor" id="src"></span>Source Code</h2>
        <div class="hl codehilite"><pre><span></span><a id="ln-1" name="ln-1" href="#ln-1"></a><span class="c">!| Module for the FTorch `torch_model` type and associated procedures.</span>
<a id="ln-2" name="ln-2" href="#ln-2"></a><span class="c">!</span>
<a id="ln-3" name="ln-3" href="#ln-3"></a><span class="c">!  * License</span>
<a id="ln-4" name="ln-4" href="#ln-4"></a><span class="c">!    FTorch is released under an MIT license.</span>
<a id="ln-5" name="ln-5" href="#ln-5"></a><span class="c">!    See the [LICENSE](https://github.com/Cambridge-ICCS/FTorch/blob/main/LICENSE)</span>
<a id="ln-6" name="ln-6" href="#ln-6"></a><span class="c">!    file for details.</span>
<a id="ln-7" name="ln-7" href="#ln-7"></a><span class="k">module </span><span class="n">ftorch_model</span>
<a id="ln-8" name="ln-8" href="#ln-8"></a><span class="w">  </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nb">c_null_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span>
<a id="ln-9" name="ln-9" href="#ln-9"></a><span class="kt">  </span><span class="k">use </span><span class="n">ftorch_devices</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="p">:</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCUDA</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kHIP</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kXPU</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kMPS</span>
<a id="ln-10" name="ln-10" href="#ln-10"></a><span class="w">  </span><span class="k">use </span><span class="n">ftorch_types</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="p">:</span><span class="w"> </span><span class="n">ftorch_int</span>
<a id="ln-11" name="ln-11" href="#ln-11"></a><span class="w">  </span><span class="k">use </span><span class="n">ftorch_tensor</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="p">:</span><span class="w"> </span><span class="n">torch_tensor</span>
<a id="ln-12" name="ln-12" href="#ln-12"></a>
<a id="ln-13" name="ln-13" href="#ln-13"></a><span class="w">  </span><span class="k">implicit none</span>
<a id="ln-14" name="ln-14" href="#ln-14"></a>
<a id="ln-15" name="ln-15" href="#ln-15"></a><span class="k">  public</span>
<a id="ln-16" name="ln-16" href="#ln-16"></a>
<a id="ln-17" name="ln-17" href="#ln-17"></a><span class="w">  </span><span class="c">!&gt; Type for holding a torch neural net (nn.Module).</span>
<a id="ln-18" name="ln-18" href="#ln-18"></a><span class="w">  </span><span class="k">type </span><span class="n">torch_model</span>
<a id="ln-19" name="ln-19" href="#ln-19"></a><span class="w">    </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">c_null_ptr</span><span class="w">  </span><span class="c">!! pointer to the neural net in memory</span>
<a id="ln-20" name="ln-20" href="#ln-20"></a><span class="w">  </span><span class="k">contains</span>
<a id="ln-21" name="ln-21" href="#ln-21"></a><span class="k">    procedure</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">print_parameters</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="n">torch_model_print_parameters</span>
<a id="ln-22" name="ln-22" href="#ln-22"></a><span class="w">    </span><span class="k">procedure</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="n">torch_model_is_training</span>
<a id="ln-23" name="ln-23" href="#ln-23"></a><span class="w">    </span><span class="k">final</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">torch_model_delete</span>
<a id="ln-24" name="ln-24" href="#ln-24"></a><span class="w">  </span><span class="k">end type </span><span class="n">torch_model</span>
<a id="ln-25" name="ln-25" href="#ln-25"></a>
<a id="ln-26" name="ln-26" href="#ln-26"></a><span class="k">contains</span>
<a id="ln-27" name="ln-27" href="#ln-27"></a>
<a id="ln-28" name="ln-28" href="#ln-28"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-29" name="ln-29" href="#ln-29"></a><span class="w">  </span><span class="c">! --- Procedures for constructing tensors</span>
<a id="ln-30" name="ln-30" href="#ln-30"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-31" name="ln-31" href="#ln-31"></a>
<a id="ln-32" name="ln-32" href="#ln-32"></a><span class="w">  </span><span class="c">!&gt; Loads a TorchScript nn.module (pre-trained PyTorch model saved with TorchScript)</span>
<a id="ln-33" name="ln-33" href="#ln-33"></a><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_model_load</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device_index</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-34" name="ln-34" href="#ln-34"></a><span class="w">                              </span><span class="n">requires_grad</span><span class="p">,</span><span class="w"> </span><span class="n">is_training</span><span class="p">)</span>
<a id="ln-35" name="ln-35" href="#ln-35"></a><span class="w">    </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="nb">c_null_char</span>
<a id="ln-36" name="ln-36" href="#ln-36"></a><span class="nb">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_model</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">out</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model</span><span class="w">    </span><span class="c">!! Returned deserialized model</span>
<a id="ln-37" name="ln-37" href="#ln-37"></a><span class="w">    </span><span class="kt">character</span><span class="p">(</span><span class="o">*</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">filename</span><span class="w">       </span><span class="c">!! Filename of saved TorchScript model</span>
<a id="ln-38" name="ln-38" href="#ln-38"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type</span><span class="w">  </span><span class="c">!! Device type the tensor will live on (`torch_kCPU` or a GPU device type)</span>
<a id="ln-39" name="ln-39" href="#ln-39"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index</span><span class="w">  </span><span class="c">!! Device index for GPU devices</span>
<a id="ln-40" name="ln-40" href="#ln-40"></a><span class="w">    </span><span class="kt">logical</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<a id="ln-41" name="ln-41" href="#ln-41"></a><span class="w">    </span><span class="kt">logical</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training</span><span class="w">    </span><span class="c">!! Whether the model is being trained, rather than evaluated</span>
<a id="ln-42" name="ln-42" href="#ln-42"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_value</span>
<a id="ln-43" name="ln-43" href="#ln-43"></a><span class="w">    </span><span class="kt">logical</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<a id="ln-44" name="ln-44" href="#ln-44"></a><span class="w">    </span><span class="kt">logical</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training_value</span><span class="w">  </span><span class="c">!! Whether the model is being trained, rather than evaluated</span>
<a id="ln-45" name="ln-45" href="#ln-45"></a>
<a id="ln-46" name="ln-46" href="#ln-46"></a><span class="w">    </span><span class="k">interface</span>
<a id="ln-47" name="ln-47" href="#ln-47"></a><span class="k">      function </span><span class="n">torch_jit_load_c</span><span class="p">(</span><span class="n">filename_c</span><span class="p">,</span><span class="w"> </span><span class="n">device_type_c</span><span class="p">,</span><span class="w"> </span><span class="n">device_index_c</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-48" name="ln-48" href="#ln-48"></a><span class="w">                                </span><span class="n">requires_grad_c</span><span class="p">,</span><span class="w"> </span><span class="n">is_training_c</span><span class="p">)</span><span class="w"> </span><span class="k">result</span><span class="p">(</span><span class="n">model_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-49" name="ln-49" href="#ln-49"></a><span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_jit_load&#39;</span><span class="p">)</span>
<a id="ln-50" name="ln-50" href="#ln-50"></a><span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_char</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span>
<a id="ln-51" name="ln-51" href="#ln-51"></a><span class="kt">        </span><span class="k">implicit none</span>
<a id="ln-52" name="ln-52" href="#ln-52"></a><span class="k">        </span><span class="kt">character</span><span class="p">(</span><span class="kt">c_char</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">filename_c</span><span class="p">(</span><span class="o">*</span><span class="p">)</span>
<a id="ln-53" name="ln-53" href="#ln-53"></a><span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">    </span><span class="kd">::</span><span class="w"> </span><span class="n">device_type_c</span>
<a id="ln-54" name="ln-54" href="#ln-54"></a><span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w">    </span><span class="kd">::</span><span class="w"> </span><span class="n">device_index_c</span>
<a id="ln-55" name="ln-55" href="#ln-55"></a><span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_c</span>
<a id="ln-56" name="ln-56" href="#ln-56"></a><span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training_c</span>
<a id="ln-57" name="ln-57" href="#ln-57"></a><span class="w">        </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">)</span><span class="w">                   </span><span class="kd">::</span><span class="w"> </span><span class="n">model_c</span>
<a id="ln-58" name="ln-58" href="#ln-58"></a><span class="w">      </span><span class="k">end function </span><span class="n">torch_jit_load_c</span>
<a id="ln-59" name="ln-59" href="#ln-59"></a><span class="w">    </span><span class="k">end interface</span>
<a id="ln-60" name="ln-60" href="#ln-60"></a>
<a id="ln-61" name="ln-61" href="#ln-61"></a><span class="w">    </span><span class="c">! Process optional arguments</span>
<a id="ln-62" name="ln-62" href="#ln-62"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">present</span><span class="p">(</span><span class="n">device_index</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<a id="ln-63" name="ln-63" href="#ln-63"></a><span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_index</span>
<a id="ln-64" name="ln-64" href="#ln-64"></a><span class="w">    </span><span class="k">else if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<a id="ln-65" name="ln-65" href="#ln-65"></a><span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span>
<a id="ln-66" name="ln-66" href="#ln-66"></a><span class="w">    </span><span class="k">else</span>
<a id="ln-67" name="ln-67" href="#ln-67"></a><span class="k">      </span><span class="n">device_index_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<a id="ln-68" name="ln-68" href="#ln-68"></a><span class="w">    </span><span class="k">endif</span>
<a id="ln-69" name="ln-69" href="#ln-69"></a>
<a id="ln-70" name="ln-70" href="#ln-70"></a><span class="k">    if</span><span class="w"> </span><span class="p">(.</span><span class="nb">not</span><span class="p">.</span><span class="w"> </span><span class="nb">present</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<a id="ln-71" name="ln-71" href="#ln-71"></a><span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">.</span><span class="n">false</span><span class="p">.</span>
<a id="ln-72" name="ln-72" href="#ln-72"></a><span class="w">    </span><span class="k">else</span>
<a id="ln-73" name="ln-73" href="#ln-73"></a><span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span>
<a id="ln-74" name="ln-74" href="#ln-74"></a><span class="w">    </span><span class="k">end if</span>
<a id="ln-75" name="ln-75" href="#ln-75"></a>
<a id="ln-76" name="ln-76" href="#ln-76"></a><span class="k">    if</span><span class="w"> </span><span class="p">(.</span><span class="nb">not</span><span class="p">.</span><span class="w"> </span><span class="nb">present</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<a id="ln-77" name="ln-77" href="#ln-77"></a><span class="k">      </span><span class="n">is_training_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">.</span><span class="n">false</span><span class="p">.</span>
<a id="ln-78" name="ln-78" href="#ln-78"></a><span class="w">    </span><span class="k">else</span>
<a id="ln-79" name="ln-79" href="#ln-79"></a><span class="k">      </span><span class="n">is_training_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">is_training</span>
<a id="ln-80" name="ln-80" href="#ln-80"></a><span class="w">    </span><span class="k">end if</span>
<a id="ln-81" name="ln-81" href="#ln-81"></a>
<a id="ln-82" name="ln-82" href="#ln-82"></a><span class="w">    </span><span class="c">! Need to append c_null_char at end of filename</span>
<a id="ln-83" name="ln-83" href="#ln-83"></a><span class="w">    </span><span class="n">model</span><span class="p">%</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch_jit_load_c</span><span class="p">(</span><span class="nb">trim</span><span class="p">(</span><span class="nb">adjustl</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span><span class="o">//</span><span class="nb">c_null_char</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-84" name="ln-84" href="#ln-84"></a><span class="w">                               </span><span class="n">device_index_value</span><span class="p">,</span><span class="w"> </span><span class="kt">logical</span><span class="p">(</span><span class="n">requires_grad_value</span><span class="p">,</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-85" name="ln-85" href="#ln-85"></a><span class="w">                               </span><span class="kt">logical</span><span class="p">(</span><span class="n">is_training_value</span><span class="p">,</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">))</span>
<a id="ln-86" name="ln-86" href="#ln-86"></a><span class="w">  </span><span class="k">end subroutine </span><span class="n">torch_model_load</span>
<a id="ln-87" name="ln-87" href="#ln-87"></a>
<a id="ln-88" name="ln-88" href="#ln-88"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-89" name="ln-89" href="#ln-89"></a><span class="w">  </span><span class="c">! --- Procedures for performing inference</span>
<a id="ln-90" name="ln-90" href="#ln-90"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-91" name="ln-91" href="#ln-91"></a>
<a id="ln-92" name="ln-92" href="#ln-92"></a><span class="w">  </span><span class="c">!&gt; Performs a forward pass of the model with the input tensors</span>
<a id="ln-93" name="ln-93" href="#ln-93"></a><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_model_forward</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">input_tensors</span><span class="p">,</span><span class="w"> </span><span class="n">output_tensors</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">)</span>
<a id="ln-94" name="ln-94" href="#ln-94"></a><span class="w">    </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span><span class="p">,</span><span class="w"> </span><span class="nb">c_loc</span>
<a id="ln-95" name="ln-95" href="#ln-95"></a><span class="nb">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_model</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model</span><span class="w">  </span><span class="c">!! Model</span>
<a id="ln-96" name="ln-96" href="#ln-96"></a><span class="w">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">),</span><span class="w"> </span><span class="k">dimension</span><span class="p">(:)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">input_tensors</span><span class="w">   </span><span class="c">!! Array of Input tensors</span>
<a id="ln-97" name="ln-97" href="#ln-97"></a><span class="w">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">),</span><span class="w"> </span><span class="k">dimension</span><span class="p">(:)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">output_tensors</span><span class="w">  </span><span class="c">!! Returned output tensors</span>
<a id="ln-98" name="ln-98" href="#ln-98"></a><span class="w">    </span><span class="kt">logical</span><span class="p">,</span><span class="w"> </span><span class="k">optional</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<a id="ln-99" name="ln-99" href="#ln-99"></a><span class="w">    </span><span class="kt">logical</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_value</span><span class="w">  </span><span class="c">!! Whether gradients need to be computed for the created tensor</span>
<a id="ln-100" name="ln-100" href="#ln-100"></a>
<a id="ln-101" name="ln-101" href="#ln-101"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="n">ftorch_int</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">i</span>
<a id="ln-102" name="ln-102" href="#ln-102"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">n_inputs</span>
<a id="ln-103" name="ln-103" href="#ln-103"></a><span class="w">    </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">)</span><span class="w">      </span><span class="kd">::</span><span class="w"> </span><span class="n">n_outputs</span>
<a id="ln-104" name="ln-104" href="#ln-104"></a><span class="w">    </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">dimension</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)),</span><span class="w"> </span><span class="k">target</span><span class="w">  </span><span class="kd">::</span><span class="w"> </span><span class="n">input_ptrs</span>
<a id="ln-105" name="ln-105" href="#ln-105"></a><span class="w">    </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">dimension</span><span class="p">(</span><span class="n">size</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)),</span><span class="w"> </span><span class="k">target</span><span class="w">  </span><span class="kd">::</span><span class="w"> </span><span class="n">output_ptrs</span>
<a id="ln-106" name="ln-106" href="#ln-106"></a>
<a id="ln-107" name="ln-107" href="#ln-107"></a><span class="w">    </span><span class="k">interface</span>
<a id="ln-108" name="ln-108" href="#ln-108"></a><span class="k">      subroutine </span><span class="n">torch_jit_model_forward_c</span><span class="p">(</span><span class="n">model_c</span><span class="p">,</span><span class="w"> </span><span class="n">input_tensors_c</span><span class="p">,</span><span class="w"> </span><span class="n">n_inputs_c</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-109" name="ln-109" href="#ln-109"></a><span class="w">                                           </span><span class="n">output_tensors_c</span><span class="p">,</span><span class="w"> </span><span class="n">n_outputs_c</span><span class="p">,</span><span class="w"> </span><span class="n">requires_grad_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-110" name="ln-110" href="#ln-110"></a><span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_jit_module_forward&#39;</span><span class="p">)</span>
<a id="ln-111" name="ln-111" href="#ln-111"></a><span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span><span class="p">,</span><span class="w"> </span><span class="kt">c_int</span>
<a id="ln-112" name="ln-112" href="#ln-112"></a><span class="kt">        </span><span class="k">implicit none</span>
<a id="ln-113" name="ln-113" href="#ln-113"></a><span class="k">        type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model_c</span>
<a id="ln-114" name="ln-114" href="#ln-114"></a><span class="w">        </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">input_tensors_c</span>
<a id="ln-115" name="ln-115" href="#ln-115"></a><span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">n_inputs_c</span>
<a id="ln-116" name="ln-116" href="#ln-116"></a><span class="w">        </span><span class="k">type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">output_tensors_c</span>
<a id="ln-117" name="ln-117" href="#ln-117"></a><span class="w">        </span><span class="kt">integer</span><span class="p">(</span><span class="kt">c_int</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">n_outputs_c</span>
<a id="ln-118" name="ln-118" href="#ln-118"></a><span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">requires_grad_c</span>
<a id="ln-119" name="ln-119" href="#ln-119"></a><span class="w">      </span><span class="k">end subroutine </span><span class="n">torch_jit_model_forward_c</span>
<a id="ln-120" name="ln-120" href="#ln-120"></a><span class="w">    </span><span class="k">end interface</span>
<a id="ln-121" name="ln-121" href="#ln-121"></a>
<a id="ln-122" name="ln-122" href="#ln-122"></a><span class="k">    </span><span class="n">n_inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
<a id="ln-123" name="ln-123" href="#ln-123"></a><span class="w">    </span><span class="n">n_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)</span>
<a id="ln-124" name="ln-124" href="#ln-124"></a>
<a id="ln-125" name="ln-125" href="#ln-125"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(.</span><span class="nb">not</span><span class="p">.</span><span class="w"> </span><span class="nb">present</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<a id="ln-126" name="ln-126" href="#ln-126"></a><span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">.</span><span class="n">false</span><span class="p">.</span>
<a id="ln-127" name="ln-127" href="#ln-127"></a><span class="w">    </span><span class="k">else</span>
<a id="ln-128" name="ln-128" href="#ln-128"></a><span class="k">      </span><span class="n">requires_grad_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span>
<a id="ln-129" name="ln-129" href="#ln-129"></a><span class="w">    </span><span class="k">end if</span>
<a id="ln-130" name="ln-130" href="#ln-130"></a>
<a id="ln-131" name="ln-131" href="#ln-131"></a><span class="w">    </span><span class="c">! Assign array of pointers to the input tensors</span>
<a id="ln-132" name="ln-132" href="#ln-132"></a><span class="w">    </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n_inputs</span>
<a id="ln-133" name="ln-133" href="#ln-133"></a><span class="w">      </span><span class="n">input_ptrs</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_tensors</span><span class="p">(</span><span class="n">i</span><span class="p">)%</span><span class="n">p</span>
<a id="ln-134" name="ln-134" href="#ln-134"></a><span class="w">    </span><span class="k">end do</span>
<a id="ln-135" name="ln-135" href="#ln-135"></a>
<a id="ln-136" name="ln-136" href="#ln-136"></a><span class="w">    </span><span class="c">! Assign array of pointers to the output tensors</span>
<a id="ln-137" name="ln-137" href="#ln-137"></a><span class="w">    </span><span class="k">do </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">n_outputs</span>
<a id="ln-138" name="ln-138" href="#ln-138"></a><span class="w">      </span><span class="n">output_ptrs</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_tensors</span><span class="p">(</span><span class="n">i</span><span class="p">)%</span><span class="n">p</span>
<a id="ln-139" name="ln-139" href="#ln-139"></a><span class="w">    </span><span class="k">end do</span>
<a id="ln-140" name="ln-140" href="#ln-140"></a>
<a id="ln-141" name="ln-141" href="#ln-141"></a><span class="k">    call </span><span class="n">torch_jit_model_forward_c</span><span class="p">(</span><span class="n">model</span><span class="p">%</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="nb">c_loc</span><span class="p">(</span><span class="n">input_ptrs</span><span class="p">),</span><span class="w"> </span><span class="n">n_inputs</span><span class="p">,</span><span class="w">       </span><span class="p">&amp;</span>
<a id="ln-142" name="ln-142" href="#ln-142"></a><span class="w">                                   </span><span class="nb">c_loc</span><span class="p">(</span><span class="n">output_ptrs</span><span class="p">),</span><span class="w"> </span><span class="n">n_outputs</span><span class="p">,</span><span class="w">              </span><span class="p">&amp;</span>
<a id="ln-143" name="ln-143" href="#ln-143"></a><span class="w">                                   </span><span class="kt">logical</span><span class="p">(</span><span class="n">requires_grad_value</span><span class="p">,</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">))</span>
<a id="ln-144" name="ln-144" href="#ln-144"></a><span class="w">  </span><span class="k">end subroutine </span><span class="n">torch_model_forward</span>
<a id="ln-145" name="ln-145" href="#ln-145"></a>
<a id="ln-146" name="ln-146" href="#ln-146"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-147" name="ln-147" href="#ln-147"></a><span class="w">  </span><span class="c">! --- Procedures for interrogating tensors</span>
<a id="ln-148" name="ln-148" href="#ln-148"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-149" name="ln-149" href="#ln-149"></a>
<a id="ln-150" name="ln-150" href="#ln-150"></a><span class="w">  </span><span class="c">!| Prints the parameters associated with a model</span>
<a id="ln-151" name="ln-151" href="#ln-151"></a><span class="w">  </span><span class="c">!  NOTE: While viewing parameters in this way can be helpful for small toy models, it will produce</span>
<a id="ln-152" name="ln-152" href="#ln-152"></a><span class="w">  </span><span class="c">!        large amounts of output for models with many, large, or high-dimensional parameters. In</span>
<a id="ln-153" name="ln-153" href="#ln-153"></a><span class="w">  </span><span class="c">!        particular, tensors of 3 or more dimensions will be represented in terms of 2D arrays.</span>
<a id="ln-154" name="ln-154" href="#ln-154"></a><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_model_print_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
<a id="ln-155" name="ln-155" href="#ln-155"></a><span class="w">    </span><span class="k">class</span><span class="p">(</span><span class="n">torch_model</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">self</span><span class="w">  </span><span class="c">!! Model to print the parameters of</span>
<a id="ln-156" name="ln-156" href="#ln-156"></a>
<a id="ln-157" name="ln-157" href="#ln-157"></a><span class="w">    </span><span class="k">interface</span>
<a id="ln-158" name="ln-158" href="#ln-158"></a><span class="k">      subroutine </span><span class="n">torch_jit_model_print_parameters_c</span><span class="p">(</span><span class="n">model_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-159" name="ln-159" href="#ln-159"></a><span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_jit_module_print_parameters&#39;</span><span class="p">)</span>
<a id="ln-160" name="ln-160" href="#ln-160"></a><span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_ptr</span>
<a id="ln-161" name="ln-161" href="#ln-161"></a><span class="kt">        </span><span class="k">implicit none</span>
<a id="ln-162" name="ln-162" href="#ln-162"></a><span class="k">        type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model_c</span>
<a id="ln-163" name="ln-163" href="#ln-163"></a><span class="w">      </span><span class="k">end subroutine </span><span class="n">torch_jit_model_print_parameters_c</span>
<a id="ln-164" name="ln-164" href="#ln-164"></a><span class="w">    </span><span class="k">end interface</span>
<a id="ln-165" name="ln-165" href="#ln-165"></a>
<a id="ln-166" name="ln-166" href="#ln-166"></a><span class="k">    call </span><span class="n">torch_jit_model_print_parameters_c</span><span class="p">(</span><span class="n">self</span><span class="p">%</span><span class="n">p</span><span class="p">)</span>
<a id="ln-167" name="ln-167" href="#ln-167"></a><span class="w">  </span><span class="k">end subroutine </span><span class="n">torch_model_print_parameters</span>
<a id="ln-168" name="ln-168" href="#ln-168"></a>
<a id="ln-169" name="ln-169" href="#ln-169"></a><span class="w">  </span><span class="c">!&gt; Determines whether a model is set up for training</span>
<a id="ln-170" name="ln-170" href="#ln-170"></a><span class="w">  </span><span class="k">function </span><span class="n">torch_model_is_training</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="k">result</span><span class="p">(</span><span class="n">is_training</span><span class="p">)</span>
<a id="ln-171" name="ln-171" href="#ln-171"></a><span class="w">    </span><span class="k">class</span><span class="p">(</span><span class="n">torch_model</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">self</span><span class="w">  </span><span class="c">!! Model to query</span>
<a id="ln-172" name="ln-172" href="#ln-172"></a><span class="w">    </span><span class="kt">logical</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training</span><span class="w">                  </span><span class="c">!! Whether the model is set up for training</span>
<a id="ln-173" name="ln-173" href="#ln-173"></a>
<a id="ln-174" name="ln-174" href="#ln-174"></a><span class="w">    </span><span class="k">interface</span>
<a id="ln-175" name="ln-175" href="#ln-175"></a><span class="k">      function </span><span class="n">torch_jit_model_is_training_c</span><span class="p">(</span><span class="n">model_c</span><span class="p">)</span><span class="w"> </span><span class="k">result</span><span class="p">(</span><span class="n">is_training_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-176" name="ln-176" href="#ln-176"></a><span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_jit_module_is_training&#39;</span><span class="p">)</span>
<a id="ln-177" name="ln-177" href="#ln-177"></a><span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_bool</span><span class="p">,</span><span class="w"> </span><span class="kt">c_ptr</span>
<a id="ln-178" name="ln-178" href="#ln-178"></a><span class="kt">        </span><span class="k">implicit none</span>
<a id="ln-179" name="ln-179" href="#ln-179"></a><span class="k">        type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model_c</span>
<a id="ln-180" name="ln-180" href="#ln-180"></a><span class="w">        </span><span class="kt">logical</span><span class="p">(</span><span class="kt">c_bool</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">is_training_c</span>
<a id="ln-181" name="ln-181" href="#ln-181"></a><span class="w">      </span><span class="k">end function </span><span class="n">torch_jit_model_is_training_c</span>
<a id="ln-182" name="ln-182" href="#ln-182"></a><span class="w">    </span><span class="k">end interface</span>
<a id="ln-183" name="ln-183" href="#ln-183"></a>
<a id="ln-184" name="ln-184" href="#ln-184"></a><span class="k">    </span><span class="n">is_training</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch_jit_model_is_training_c</span><span class="p">(</span><span class="n">self</span><span class="p">%</span><span class="n">p</span><span class="p">)</span>
<a id="ln-185" name="ln-185" href="#ln-185"></a><span class="w">  </span><span class="k">end function </span><span class="n">torch_model_is_training</span>
<a id="ln-186" name="ln-186" href="#ln-186"></a>
<a id="ln-187" name="ln-187" href="#ln-187"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-188" name="ln-188" href="#ln-188"></a><span class="w">  </span><span class="c">! --- Procedures for deallocating models</span>
<a id="ln-189" name="ln-189" href="#ln-189"></a><span class="w">  </span><span class="c">! ============================================================================</span>
<a id="ln-190" name="ln-190" href="#ln-190"></a>
<a id="ln-191" name="ln-191" href="#ln-191"></a><span class="w">  </span><span class="c">!&gt; Deallocates a TorchScript model</span>
<a id="ln-192" name="ln-192" href="#ln-192"></a><span class="w">  </span><span class="k">subroutine </span><span class="n">torch_model_delete</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<a id="ln-193" name="ln-193" href="#ln-193"></a><span class="w">    </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="nb">c_associated</span><span class="p">,</span><span class="w"> </span><span class="nb">c_null_ptr</span>
<a id="ln-194" name="ln-194" href="#ln-194"></a><span class="nb">    </span><span class="k">type</span><span class="p">(</span><span class="n">torch_model</span><span class="p">),</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">inout</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model</span><span class="w">  </span><span class="c">!! Torch Model to deallocate</span>
<a id="ln-195" name="ln-195" href="#ln-195"></a>
<a id="ln-196" name="ln-196" href="#ln-196"></a><span class="w">    </span><span class="k">interface</span>
<a id="ln-197" name="ln-197" href="#ln-197"></a><span class="k">      subroutine </span><span class="n">torch_jit_model_delete_c</span><span class="p">(</span><span class="n">model_c</span><span class="p">)</span><span class="w"> </span><span class="p">&amp;</span>
<a id="ln-198" name="ln-198" href="#ln-198"></a><span class="w">          </span><span class="k">bind</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;torch_jit_module_delete&#39;</span><span class="p">)</span>
<a id="ln-199" name="ln-199" href="#ln-199"></a><span class="w">        </span><span class="k">use</span><span class="p">,</span><span class="w"> </span><span class="k">intrinsic</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="nb">iso_c_binding</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="kt">c_ptr</span>
<a id="ln-200" name="ln-200" href="#ln-200"></a><span class="kt">        </span><span class="k">implicit none</span>
<a id="ln-201" name="ln-201" href="#ln-201"></a><span class="k">        type</span><span class="p">(</span><span class="kt">c_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">intent</span><span class="p">(</span><span class="n">in</span><span class="p">)</span><span class="w"> </span><span class="kd">::</span><span class="w"> </span><span class="n">model_c</span>
<a id="ln-202" name="ln-202" href="#ln-202"></a><span class="w">      </span><span class="k">end subroutine </span><span class="n">torch_jit_model_delete_c</span>
<a id="ln-203" name="ln-203" href="#ln-203"></a><span class="w">    </span><span class="k">end interface</span>
<a id="ln-204" name="ln-204" href="#ln-204"></a>
<a id="ln-205" name="ln-205" href="#ln-205"></a><span class="w">    </span><span class="c">! Call the destructor, if it hasn&#39;t already been called</span>
<a id="ln-206" name="ln-206" href="#ln-206"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="nb">c_associated</span><span class="p">(</span><span class="n">model</span><span class="p">%</span><span class="n">p</span><span class="p">))</span><span class="w"> </span><span class="k">then</span>
<a id="ln-207" name="ln-207" href="#ln-207"></a><span class="k">      call </span><span class="n">torch_jit_model_delete_c</span><span class="p">(</span><span class="n">model</span><span class="p">%</span><span class="n">p</span><span class="p">)</span>
<a id="ln-208" name="ln-208" href="#ln-208"></a><span class="w">      </span><span class="n">model</span><span class="p">%</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">c_null_ptr</span>
<a id="ln-209" name="ln-209" href="#ln-209"></a><span class="nb">    </span><span class="k">end if</span>
<a id="ln-210" name="ln-210" href="#ln-210"></a><span class="k">  end subroutine </span><span class="n">torch_model_delete</span>
<a id="ln-211" name="ln-211" href="#ln-211"></a>
<a id="ln-212" name="ln-212" href="#ln-212"></a><span class="k">end module </span><span class="n">ftorch_model</span>
</pre></div>

      </section>
    </div>
  </div>

      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2026 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>