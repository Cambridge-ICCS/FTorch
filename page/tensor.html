<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="A library for coupling (Py)Torch machine learning models to Fortran">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>Tensor API &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>Tensor API</h1>
    <div class="container p-2 mb-4 bg-light border rounded-3">
      <div class="row align-items-center justify-content-between">
        <div class="col">
          <ul class="list-inline" style="margin-bottom:0px; display:inline">
          </ul>
        </div>
        <div class="col">
          <nav aria-label="breadcrumb">
            <ol class="breadcrumb justify-content-end mb-0">
                <li class="breadcrumb-item"><a href='index.html'>User Guide</a></li>
              <li class="breadcrumb-item active" aria-current="page">Tensor API</li>
            </ol>
          </nav>
        </div>
      </div>
    </div>
  </div>

  <div class="row">
      <div class="col-3">
        <div class="card card-body bg-light" id="sidebar-toc">
          <ul class="nav flex-column align-items">
            <li class="nav-item">
              <a class="nav-link" href="index.html">User Guide</a>
            </li>
          </ul>
          <hr>
          <nav class="nav nav-pills flex-column">
              <a class="nav-link" href="LICENSE.html">FTorch License</a>
              <a class="nav-link" href="archive.html">News archive</a>
              <a class="nav-link" href="changelog.html">FTorch Changelog</a>
              <a class="nav-link" href="cmake.html">Installation and Build Process</a>
              <a class="nav-link" href="developer.html">Developer Guide</a>
              <a class="nav-link" href="examples.html">Examples</a>
              <a class="nav-link" href="gpu.html">GPU Support</a>
              <a class="nav-link" href="hpc.html">Guidance for use in High Performance Computing (HPC)</a>
              <a class="nav-link" href="offline.html">Offline training</a>
              <a class="nav-link" href="online.html">Online training</a>
              <a class="nav-link" href="presentations.html">Presentations</a>
              <a class="nav-link active disabled" href="tensor.html">Tensor API</a>
              <a class="nav-link" href="testing.html">FTorch test suite</a>
              <a class="nav-link" href="transposing.html">When to transpose data</a>
              <a class="nav-link" href="troubleshooting.html">Troubleshooting</a>
              <a class="nav-link" href="updates.html">Recent API Changes</a>
          </nav>
        </div>
      </div>

    <div class="col-9" id='text'>
      <div class="toc">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#procedures">Procedures</a><ul>
<li><a href="#constructors">Constructors</a></li>
<li><a href="#tensor-interrogation">Tensor interrogation</a></li>
<li><a href="#tensor-deallocation">Tensor deallocation</a></li>
<li><a href="#tensor-manipulation">Tensor manipulation</a></li>
<li><a href="#operator-overloading">Operator overloading</a><ul>
<li><a href="#overloaded-assignment-operator">Overloaded assignment operator</a></li>
</ul>
</li>
<li><a href="#other-operators-acting-on-tensors">Other operators acting on tensors</a></li>
</ul>
</li>
<li><a href="#examples">Examples</a></li>
</ul>
</div>
<h2 id="overview">Overview</h2>
<p>FTorch provides a <code>torch_tensor</code> derived type, which exposes the functionality
of the <code>torch::Tensor</code> C++ class. The interface is designed to be familiar to
Fortran programmers, whilst retaining strong similarity with <code>torch::Tensor</code> and
the <code>torch.Tensor</code> Python class.</p>
<p>Under the hood, the <code>torch_tensor</code> type holds a pointer to a <code>torch::Tensor</code>
object in C++ (implemented using <code>c_ptr</code> from the <code>iso_c_binding</code> intrinsic
module). This allows us to avoid unnecessary data copies between C++ and
Fortran.</p>
<h2 id="procedures">Procedures</h2>
<h3 id="constructors">Constructors</h3>
<p>We provide several subroutines for constructing <code>torch_tensor</code> objects. These
include:</p>
<ul>
<li><code>torch_tensor_empty</code>, which allocates memory for the <code>torch_tensor</code>, but does
  not set any values.</li>
<li><code>torch_tensor_zeros</code>, which creates a <code>torch_tensor</code> whose values are
  uniformly zero.</li>
<li><code>torch_tensor_ones</code>, which creates a <code>torch_tensor</code> whose values are
  uniformly one.</li>
<li><code>torch_tensor_from_array</code>, which allows the user to create a <code>torch_tensor</code>
  with the same rank, shape, and data type as a given Fortran array. Note that
  the data is <em>not</em> copied - the tensor data points to the Fortran array,
  meaning the array must have been declared with the <code>target</code> property. The
  array will continue to be pointed to even when operations are applied to the
  tensor, so this subroutine can be used 'in advance' to set up an array for
  outputting data. <code>torch_tensor_from_array</code> may be called with or without the
  <code>layout</code> argument - an array which specifies the order in which indices should
  be looped over. The default <code>layout</code> is <code>[1,2,...,n]</code> implies that data will
  be read into the same indices by Torch. (See the
  <a href="pages/transposing.html">transposing user guide page</a> for more details.</li>
</ul>
<p>It is <em>compulsory</em> to call one of the constructors before interacting with it in
any of the ways described in the following. Each of the constructors sets the
pointer attribute of the <code>torch_tensor</code>; without this being set, most of the
other operations are meaningless.</p>
<h3 id="tensor-interrogation">Tensor interrogation</h3>
<p>We provide several subroutines for interrogating <code>torch_tensor</code> objects. These
include:</p>
<ul>
<li><code>torch_tensor_get_rank</code>, which determines the rank (i.e., dimensionality) of
  the tensor.</li>
<li><code>torch_tensor_get_shape</code>, which determines the shape (i.e., extent in each
  dimension) of the tensor.</li>
<li><code>torch_tensor_get_dtype</code>, which determines the data type of the tensor in
  terms of the enums <code>torch_kInt8</code>, <code>torch_kFloat32</code>, etc.</li>
<li><code>torch_tensor_get_device_type</code>, which determines the device type that the
  tensor resides on in terms of the enums <code>torch_kCPU</code>, <code>torch_kCUDA</code>,
  <code>torch_kXPU</code>, etc.</li>
<li><code>torch_tensor_get_device_index</code>, which determines the index of the device that
  the tensor resides on as an integer. For a CPU device, this index should be
  set to -1 (the default). For GPU devices, the index should be non-negative
  (defaulting to 0).</li>
</ul>
<p>Procedures for interrogation are implemented as methods as well as stand-alone
procedures. For example, <code>tensor%get_rank</code> can be used in place of
<code>torch_tensor_get_rank</code>, omitting the first argument (which would be the tensor
itself). The naming pattern is similar for the other methods (simply drop the
preceding <code>torch_tensor_</code>).</p>
<h3 id="tensor-deallocation">Tensor deallocation</h3>
<p>We provide a subroutine for deallocating the memory associated with a
<code>torch_tensor</code> object: <code>torch_tensor_delete</code>. An interface is provided such that
this can also be applied to arrays of tensors. Calling this subroutine manually
is optional as it is called as a destructor when the <code>torch_tensor</code> goes out of
scope anyway.</p>
<h3 id="tensor-manipulation">Tensor manipulation</h3>
<p>We provide the following subroutines for manipulating the data values associated
with a <code>torch_tensor</code> object:</p>
<ul>
<li><code>torch_tensor_zero</code> (aliased as class method <code>torch_tensor%zero</code>), which
  sets all the data entries associated with a tensor to zero.</li>
</ul>
<h3 id="operator-overloading">Operator overloading</h3>
<p>Mathematical operators involving Tensors are overloaded, so that we can compute
expressions involving outputs from one or more ML models.</p>
<p>Whilst it's possible to import such functionality with a bare</p>
<div class="codehilite"><pre><span></span><code><span class="k">use </span><span class="n">ftorch</span>
</code></pre></div>

<p>statement, the best practice is to import specifically the operators that you
wish to use. Note that the assignment operator <code>=</code> has a slightly different
notation:</p>
<div class="codehilite"><pre><span></span><code><span class="n">use</span><span class="w"> </span><span class="n">ftorch</span><span class="p">,</span><span class="w"> </span><span class="nl">only:</span><span class="w"> </span><span class="n">assignment</span><span class="p">(</span><span class="o">=</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">+</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">-</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">*</span><span class="p">),</span><span class="w"> </span><span class="o">&amp;</span>
<span class="w">  </span><span class="n">operator</span><span class="p">(</span><span class="o">/</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">**</span><span class="p">)</span>
</code></pre></div>

<h4 id="overloaded-assignment-operator">Overloaded assignment operator</h4>
<p>Particular care should be taken with the overloaded assignment operator.
Whenever you execute code involving <code>torch_tensor</code>s on each side of an equals
sign, the overloaded assignment operator should be triggered. As such, if you
aren't using the bare <code>use ftorch</code> import then you should ensure you specify
<code>use ftorch, only: assignment(=)</code> (as well as any other module members you
require).</p>
<p>For a straightforward assignment of two <code>torch_tensor</code>s <code>a</code> and <code>b</code>,</p>
<div class="codehilite"><pre><span></span><code><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span>
</code></pre></div>

<p>the overloaded assignment operator is called once.</p>
<p>For overloaded operators the situation is more complex. Consider the overloaded
addition operator (the same applies for the rest). When we execute the line</p>
<div class="codehilite"><pre><span></span><code><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span>
</code></pre></div>

<p>the addition is evaluated first. It is implemented as a Fortran function and its
return value is an <em>intermediate</em> tensor. The setup is such that this is created
using <code>torch_tensor_empty</code> under the hood (inheriting all the properties of the
tensors being added<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>). Following this, the intermediate tensor is assigned
to <code>c</code>. Finally, the finalizer for <code>torch_tensor</code> is called for the intermediate
tensor because it goes out of scope.</p>
<p>Similarly as above, in the case where you have some function <code>func</code> that returns
a <code>torch_tensor</code>, an intermediate <code>torch_tensor</code> will be created, assigned, and
destroyed because the call will have the form</p>
<div class="codehilite"><pre><span></span><code><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">func</span><span class="p">()</span>
</code></pre></div>

<h3 id="other-operators-acting-on-tensors">Other operators acting on tensors</h3>
<p>We have also exposed the operators for taking the sum or mean over the entries
in a tensor, which can be achieved with the subroutines <code>torch_tensor_sum</code> and
in <code>torch_tensor_mean</code>, respectively.</p>
<h2 id="examples">Examples</h2>
<p>For a concrete example of how to construct, interrogate, manipulate, and delete
Torch tensors, see the
<a href="https://github.com/Cambridge-ICCS/FTorch/tree/main/examples/1_Tensor">tensor manipulation worked example</a>.</p>
<p>For an example of how to compute mathematical expressions involving Torch
tensors, see the
<a href="https://github.com/Cambridge-ICCS/FTorch/tree/main/examples/6_Autograd">autograd worked example</a>.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:1">
<p>Note: In most cases, these should be the same, so that the operator makes
sense. In the case of the <code>requires_grad</code> property, the values might differ, and
the result should be the logical <code>.and.</code> of the two values.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
    </div>
  </div>
      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2026 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>