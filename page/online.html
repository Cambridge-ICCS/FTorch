<!-- -*- mode: jinja2 -*- -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta name="description" content="A library for coupling (Py)Torch machine learning models to Fortran">
    <meta name="author" content="ICCS Cambridge" >
    <link rel="icon" href="../favicon.png">

    <title>Online training &ndash; FTorch</title>

    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <!-- Font Awesome -->
    <link href="../css/fontawesome.min.css" rel="stylesheet">
    <link href="../css/brands.min.css" rel="stylesheet">
    <link href="../css/regular.min.css" rel="stylesheet">
    <link href="../css/solid.min.css" rel="stylesheet">
    <link href="../css/v4-font-face.min.css" rel="stylesheet">
    <link href="../css/v4-shims.min.css" rel="stylesheet">
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
      });
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async
            integrity="sha256-DViIOMYdwlM/axqoGDPeUyf0urLoHMN4QACBKyB58Uw=" crossorigin="anonymous"></script>
    <!-- Other scripts and stylesheets -->
    <link href="../css/local.css" rel="stylesheet">
    <link href="../css/pygments.css" rel="stylesheet">
    <script src="../js/svg-pan-zoom.min.js"></script>
  </head>

  <body>

    <!-- Fixed navbar -->
    <div class="container-fluid mb-sm-4 mb-xl-2">
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
          <a class="navbar-brand" href="../index.html">FTorch </a>
          <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar"
                  aria-expanded="false" aria-controls="navbar" aria-label="Toggle navigation">
                  <span class="navbar-toggler-icon">
          </button>

          <div id="navbar" class="navbar-collapse collapse">
            <ul class="navbar-nav">
                <li class="nav-item"><a class="nav-link" href="index.html">User Guide</a></li>
                  <li class="nav-item">
                    <a class="nav-link" href="../lists/files.html">Source Files</a>
                  </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/modules.html">Modules</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/procedures.html">Procedures</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="../lists/types.html">Derived Types</a>
                </li>
            </ul>
              <div class="d-flex align-items-end flex-grow-1">
                <form action="../search.html" role="search" class="ms-auto">
                  <input type="text" class="form-control" aria-label="Search" placeholder="Search" name="q" id="tipue_search_input" autocomplete="off" required>
                </form>
              </div>
          </div><!--/.nav-collapse -->
        </div>
      </nav>
    </div>

    <div class="container">
  <div class="row">
    <h1>Online training</h1>
    <div class="container p-2 mb-4 bg-light border rounded-3">
      <div class="row align-items-center justify-content-between">
        <div class="col">
          <ul class="list-inline" style="margin-bottom:0px; display:inline">
          </ul>
        </div>
        <div class="col">
          <nav aria-label="breadcrumb">
            <ol class="breadcrumb justify-content-end mb-0">
                <li class="breadcrumb-item"><a href='index.html'>User Guide</a></li>
              <li class="breadcrumb-item active" aria-current="page">Online training</li>
            </ol>
          </nav>
        </div>
      </div>
    </div>
  </div>

  <div class="row">
      <div class="col-3">
        <div class="card card-body bg-light" id="sidebar-toc">
          <ul class="nav flex-column align-items">
            <li class="nav-item">
              <a class="nav-link" href="index.html">User Guide</a>
            </li>
          </ul>
          <hr>
          <nav class="nav nav-pills flex-column">
              <a class="nav-link" href="LICENSE.html">FTorch License</a>
              <a class="nav-link" href="archive.html">News archive</a>
              <a class="nav-link" href="changelog.html">FTorch Changelog</a>
              <a class="nav-link" href="cmake.html">Installation and Build Process</a>
              <a class="nav-link" href="developer.html">Developer Guide</a>
              <a class="nav-link" href="examples.html">Examples</a>
              <a class="nav-link" href="gpu.html">GPU Support</a>
              <a class="nav-link" href="hpc.html">Guidance for use in High Performance Computing (HPC)</a>
              <a class="nav-link" href="offline.html">Offline training</a>
              <a class="nav-link active disabled" href="online.html">Online training</a>
              <a class="nav-link" href="presentations.html">Presentations</a>
              <a class="nav-link" href="tensor.html">Tensor API</a>
              <a class="nav-link" href="testing.html">FTorch test suite</a>
              <a class="nav-link" href="transposing.html">When to transpose data</a>
              <a class="nav-link" href="troubleshooting.html">Troubleshooting</a>
              <a class="nav-link" href="updates.html">Recent API Changes</a>
          </nav>
        </div>
      </div>

    <div class="col-9" id='text'>
      <div class="toc">
<ul>
<li><a href="#current-state">Current state</a><ul>
<li><a href="#operator-overloading">Operator overloading</a></li>
<li><a href="#the-requires_grad-property">The requires_grad property</a></li>
<li><a href="#backpropagation">Backpropagation</a><ul>
<li><a href="#retain_graph-argument">retain_graph argument</a></li>
<li><a href="#zeroing-gradients">Zeroing gradients</a></li>
<li><a href="#extracting-gradients">Extracting gradients</a></li>
</ul>
</li>
<li><a href="#optimisation">Optimisation</a></li>
<li><a href="#loss-functions">Loss functions</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="current-state">Current state</h2>
<p>FTorch has supported offline training of ML models for some time (see the
<a href="offline.html">offline training user guide page</a> for details). We are
currently working on extending its functionality to support online training,
too. This will involve exposing the backpropagation and optimization
functionalities of PyTorch/LibTorch.</p>
<p>In the following, we document a workplan of the related functionality. Each step
below will be updated upon completion.</p>
<h3 id="operator-overloading">Operator overloading</h3>
<p>Mathematical operators involving Tensors are overloaded, so that we can compute
expressions involving outputs from one or more ML models. For more information
on this, see the [tensor API][pages/tensor.html] documentation page.</p>
<p>Whilst it's possible to import such functionality with a bare</p>
<div class="codehilite"><pre><span></span><code><span class="k">use </span><span class="n">ftorch</span>
</code></pre></div>

<p>statement, the best practice is to import specifically the operators that you
wish to use. Note that the assignment operator <code>=</code> has a slightly different
notation:</p>
<div class="codehilite"><pre><span></span><code><span class="k">use </span><span class="n">ftorch</span><span class="p">,</span><span class="w"> </span><span class="k">only</span><span class="p">:</span><span class="w"> </span><span class="n">assignment</span><span class="p">(</span><span class="o">=</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">+</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">-</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">*</span><span class="p">),</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">  </span><span class="n">operator</span><span class="p">(</span><span class="o">/</span><span class="p">),</span><span class="w"> </span><span class="n">operator</span><span class="p">(</span><span class="o">**</span><span class="p">)</span>
</code></pre></div>

<p>If you would like to make use of scalar multiplication or scalar division, this
can be achieved by setting the scalar as a rank-1 <code>torch_tensor</code> with a single
entry. For example:</p>
<div class="codehilite"><pre><span></span><code><span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">multiplier</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mf">3.0_wp</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span>
</code></pre></div>

<p>For a concrete example of how to compute mathematical expressions involving
Torch tensors, see the
<a href="https://github.com/Cambridge-ICCS/FTorch/tree/main/examples/8_Autograd">autograd worked example</a>.</p>
<h3 id="the-requires_grad-property">The <code>requires_grad</code> property</h3>
<p>For Tensors that you would like to differentiate with respect to, be sure to
set the <code>requires_grad</code> optional argument to <code>.true.</code> when you construct it.</p>
<h3 id="backpropagation">Backpropagation</h3>
<p>Having defined some tensors with the <code>requires_grad</code> property set to <code>.true.</code>
and computed another tensor in terms of an expression involving these, we can
compute gradients of that tensor with respect to those that it depends on. This
is achieved using the <code>torch_tensor_backward</code> subroutine. For example, for
input tensors <code>a</code> and <code>b</code> and an output tensor <code>Q</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">in_data1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_layout</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                             </span><span class="n">requires_grad</span><span class="o">=</span><span class="p">.</span><span class="n">true</span><span class="p">.)</span>
<span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">in_data2</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_layout</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">,</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">                             </span><span class="n">requires_grad</span><span class="o">=</span><span class="p">.</span><span class="n">true</span><span class="p">.)</span>
<span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">out_data1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_layout</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span>

<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span>

<span class="k">call </span><span class="n">torch_tensor_backward</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
</code></pre></div>

<p>Following the example code above, we can extract gradients of <code>Q</code> with respect
to <code>a</code> and/or <code>b</code>. To do this, we can use the <code>torch_tensor_get_gradient</code>
subroutine. That is, for tensors <code>dQda</code> and <code>dQdb</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">dQda</span><span class="p">,</span><span class="w"> </span><span class="n">out_data2</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_layout</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span>
<span class="k">call </span><span class="n">torch_tensor_get_gradient</span><span class="p">(</span><span class="n">dQda</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">)</span>

<span class="k">call </span><span class="n">torch_tensor_from_array</span><span class="p">(</span><span class="n">dQdb</span><span class="p">,</span><span class="w"> </span><span class="n">out_data3</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_layout</span><span class="p">,</span><span class="w"> </span><span class="n">torch_kCPU</span><span class="p">)</span>
<span class="k">call </span><span class="n">torch_tensor_get_gradient</span><span class="p">(</span><span class="n">dQdb</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
</code></pre></div>

<h4 id="retain_graph-argument"><code>retain_graph</code> argument</h4>
<p>If you wish to call the backpropagation operator multiple times then you may
need to make use of the <code>retain_graph</code> argument for <code>torch_tensor_backward</code>.
This argument accepts logical values and defaults to <code>.false.</code>, for consistency
with PyTorch and LibTorch. According to the
<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html">PyTorch docs</a>,
<code>retain_graph=.true.</code> will not be needed in most cases, but it's useful to have
for the cases where it is.</p>
<h4 id="zeroing-gradients">Zeroing gradients</h4>
<p>Having computed gradients of one tensor with respect to its dependencies,
suppose you wish to compute gradients of another tensor. Since the gradient
values associated with each dependency are accumulated, you should zero the
gradients before computing the next gradient. This can be achieved using the
<code>torch_tensor_zero_grad</code> subroutine.</p>
<p>Following the example code above:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span>
<span class="n">P</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span>

<span class="k">call </span><span class="n">torch_tensor_backward</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>

<span class="c">! ...</span>

<span class="k">call </span><span class="n">torch_tensor_zero_grad</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">call </span><span class="n">torch_tensor_zero_grad</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">call </span><span class="n">torch_tensor_backward</span><span class="p">(</span><span class="n">P</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="o">=</span><span class="p">.</span><span class="n">true</span><span class="p">.)</span>

<span class="c">! ...</span>
</code></pre></div>

<h4 id="extracting-gradients">Extracting gradients</h4>
<p>Note that <code>torch_tensor_get_gradient</code> must be called after every call to
<code>torch_tensor_backward</code> or <code>torch_tensor_zero_grad</code>, even if the gradient for
the same tensor is being extracted into the same array. This is due to the way
that pointers are handled on the C++ side.</p>
<h3 id="optimisation">Optimisation</h3>
<p><em>Not yet implemented.</em></p>
<h3 id="loss-functions">Loss functions</h3>
<p><em>Not yet implemented.</em></p>
    </div>
  </div>
      <hr>
    </div> <!-- /container -->
    <footer>
      <div class="container">
        <div class="row justify-content-between">
          <div class="col">
            <p>
              FTorch
 was developed by ICCS Cambridge<br>              &copy; 2025 <a rel="license" href="https://opensource.org/licenses/MIT">MIT</a>
</p>
          </div>
          <div class="col">
            <p class="text-end">
              Documentation generated by
              <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            </p>
          </div>
        </div>
        <br>
      </div> <!-- /container -->
    </footer>
  </body>
</html>